BEGIN TRANSACTION;
CREATE TABLE IF NOT EXISTS "algorithms" (
	"id"	INTEGER NOT NULL,
	"alg_name"	VARCHAR(64) UNIQUE,
	"web_name"	VARCHAR(64),
	"alg_typ"	VARCHAR(64),
	"config"	TEXT,
	"lib"	VARCHAR(64),
	PRIMARY KEY("id")
);
CREATE TABLE IF NOT EXISTS "experiments" (
	"id"	INTEGER NOT NULL,
	"idu"	INTEGER,
	"alg_name"	VARCHAR(64),
	"alg_config"	REAL,
	"filter_name"	VARCHAR(64),
	"filter_config"	TEXT,
	"data"	VARCHAR(128),
	"result"	TEXT,
	"starttime"	INTEGER,
	"endtime"	INTEGER,
	"state"	INTEGER,
	"exp_config"	TEXT,
	FOREIGN KEY("idu") REFERENCES "users"("id"),
	PRIMARY KEY("id"),
	FOREIGN KEY("alg_name") REFERENCES "algorithms"("alg_name")
);
CREATE TABLE IF NOT EXISTS "filters" (
	"id"	INTEGER,
	"filter_name"	VARCHAR(64) UNIQUE,
	"web_name"	VARCHAR(64),
	"filter_typ"	VARCHAR(64),
	"config"	TEXT,
	"lib"	VARCHAR(64),
	PRIMARY KEY("id")
);
INSERT INTO "algorithms" VALUES (1,'sklearn.linear_model.LinearRegression','Linear Regression','Regression','{"fit_intercept":{"type":"boolean","default":true,"help":"whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations (e.g. data is expected to be already centered)."},"normalize":{"type":"boolean","default":false,"help":"This parameter is ignored when fit_intercept is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use sklearn.preprocessing.StandardScaler before calling fit on an estimator with normalize=False."}}','sklearn');
INSERT INTO "algorithms" VALUES (2,'sklearn.ensemble.BaggingRegressor','Bagging','Regression','{"base_estimator":{"type":"ensemble","default":"sklearn.tree.DecisionTreeRegressor","help":"The base estimator to fit on random subsets of the dataset. If None, then the base estimator is a decision tree."},"n_estimators":{"type":"integer","default":"10","help":"The number of base estimators in the ensemble.","min":2},"max_samples":{"type":"integer","default":1,"help":"The number of samples to draw from X to train each base estimator.","min":1},"max_features":{"type":"integer","default":1,"help":"The number of features to draw from X to train each base estimator.","min":1},"bootstrap":{"type":"boolean","default":true,"help":"Whether samples are drawn with replacement. If False, sampling without replacement is performed."},"bootstrap_features":{"type":"boolean","default":false,"help":"Whether features are drawn with replacement."},"oob_score":{"type":"boolean","default":false,"help":"Whether to use out-of-bag samples to estimate the generalization error."},"warm_start":{"type":"boolean","default":false,"help":"When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new ensemble."},"random_state":{"type":"integer","default":null,"help":"The seed used by the random number generator.","min":0,"max":4294967295}}','sklearn');
INSERT INTO "algorithms" VALUES (3,'sklearn.ensemble.BaggingClassifier','Bagging','Classification','{"base_estimator":{"type":"ensemble","default":"sklearn.tree.DecisionTreeClassifier","help":"The base estimator to fit on random subsets of the dataset. If None, then the base estimator is a decision tree."},"n_estimators":{"type":"integer","default":"10","help":"The number of base estimators in the ensemble.","min":2},"max_samples":{"type":"integer","default":1,"help":"The number of samples to draw from X to train each base estimator.","min":1},"max_features":{"type":"integer","default":1,"help":"The number of features to draw from X to train each base estimator.","min":1},"bootstrap":{"type":"boolean","default":true,"help":"Whether samples are drawn with replacement. If False, sampling without replacement is performed."},"bootstrap_features":{"type":"boolean","default":false,"help":"Whether features are drawn with replacement."},"oob_score":{"type":"boolean","default":false,"help":"Whether to use out-of-bag samples to estimate the generalization error."},"warm_start":{"type":"boolean","default":false,"help":"When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new ensemble."},"random_state":{"type":"integer","default":null,"help":"The seed used by the random number generator.","min":0,"max":4294967295}}','sklearn');
INSERT INTO "algorithms" VALUES (4,'sklearn.neighbors.KNeighborsClassifier','KNN','Classification','{"n_neighbors":{"type":"integer","default":5,"help":"Number of neighbors to use by default for kneighbors queries.","min":1},"weights":{"type":"string","default":"uniform","help":"weight function used in prediction. Possible values:‘uniform’ : uniform weights. All points in each neighborhood are weighted equally.‘distance’ : weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away.","options":["uniform","distance"]},"algorithm":{"type":"string","default":"auto","help":"Algorithm used to compute the nearest neighbors:‘ball_tree’ will use BallTree.‘kd_tree’ will use KDTree. ‘brute’ will use a brute-force search. ‘auto’ will attempt to decide the most appropriate algorithm based on the values passed to fit method.","options":["auto","ball_tree","kd_tree","brute"]},"p":{"type":"integer","default":2,"help":"Power parameter for the Minkowski metric. When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.","min":1}}','sklearn');
INSERT INTO "algorithms" VALUES (5,'weka.classifiers.trees.J48','Decision Tree (Weka)','Classification','{"unpruned":{"type":"boolean","default":false,"help":"Use unpruned tree.","command":"-U"},"collapse":{"type":"boolean","default":false,"help":"Do not collapse tree.","command":"-O"},"pruning_confidence":{"type":"float","default":0.25,"help":"Set confidence threshold for pruning.","min":1e-16,"max":10000000000000000,"command":"-C"},"minimum_n_instances":{"type":"integer","default":2,"help":"Set minimum number of instances per leaf.","min":1,"max":10000000000000000,"command":"-M"},"reduced_error_pruning":{"type":"boolean","default":false,"help":"Use reduced error pruning.","command":"-R"},"number_folds":{"type":"integer","default":null,"help":"Set number of folds for reduced error pruning. One fold is used as pruning set.","min":2,"max":10000000000000000,"command":"-N"},"binary_split":{"type":"boolean","default":false,"help":"Use binary splits only.","command":"-B"},"not_subtree_raising":{"type":"boolean","default":false,"help":"Don''t perform subtree raising.","command":"-S"},"not_clean_up":{"type":"boolean","default":false,"help":"Do not clean up after the tree has been built.","command":"-L"},"laplace":{"type":"boolean","default":false,"help":"Laplace smoothing for predicted probabilities.","command":"-A"},"not_MDL":{"type":"boolean","default":false,"help":"Do not use MDL correction for info gain on numeric attributes.","command":"-J"},"not_splitpoint_act_value":{"type":"boolean","default":false,"help":"Do not make split point actual value.","command":"-doNotMakeSplitPointActualValue"},"seed":{"type":"integer","default":null,"help":"Seed for random data shuffling.","min":-2147483647,"max":2147483647,"command":"-Q"}}','weka');
INSERT INTO "algorithms" VALUES (6,'sklearn.linear_model.LogisticRegression','Logistic Regression','Classification','{"penalty":{"type":"string","default":"l2","help":"Used to specify the norm used in the penalization. The ‘newton-cg’, ‘sag’ and ‘lbfgs’ solvers support only l2 penalties. ‘elasticnet’ is only supported by the ‘saga’ solver. If ‘none’ (not supported by the liblinear solver), no regularization is applied.","options":["l2","l1","elasticnet","none"]},"tol":{"type":"float","default":0.0001,"help":"Tolerance for stopping criteria.","min":1e-64,"max":1e+64},"C":{"type":"float","default":1,"help":"Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization.","min":0,"max":1e+64},"fit_intercept":{"type":"boolean","default":true,"help":"Specifies if a constant (a.k.a. bias or intercept) should be added to the decision function."},"class_weight":{"type":"string","default":"","help":"The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y)).","options":["","balanced"]},"solver":{"type":"string","default":"liblinear","help":"Algorithm to use in the optimization problem.\n\nFor small datasets, ‘liblinear’ is a good choice, whereas ‘sag’ and ‘saga’ are faster for large ones.\nFor multiclass problems, only ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’ handle multinomial loss; ‘liblinear’ is limited to one-versus-rest schemes.\n‘newton-cg’, ‘lbfgs’, ‘sag’ and ‘saga’ handle L2 or no penalty\n‘liblinear’ and ‘saga’ also handle L1 penalty\n''saga’ also supports ‘elasticnet’ penalty\n‘liblinear’ does not handle no penalty","options":["liblinear","newton-cg","lbfgs","sag","saga"]},"max_iter":{"type":"integer","default":100,"help":"Maximum number of iterations taken for the solvers to converge.","min":1},"multi_class":{"type":"string","default":"ovr","help":"If the option chosen is ‘ovr’, then a binary problem is fit for each label. For ‘multinomial’ the loss minimised is the multinomial loss fit across the entire probability distribution, even when the data is binary. ‘multinomial’ is unavailable when solver=’liblinear’. ‘auto’ selects ‘ovr’ if the data is binary, or if solver=’liblinear’, and otherwise selects ‘multinomial’.","options":["ovr","multinominal","auto"]},"warm_start":{"type":"boolean","default":false,"help":"When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. Useless for liblinear solver."},"random_state":{"type":"integer","default":null,"help":"The seed of the pseudo random number generator to use when shuffling the data.","min":0,"max":4294967295}}','sklearn');
INSERT INTO "algorithms" VALUES (7,'weka.classifiers.meta.RotationForest','Rotation Forest','Mixed','{"number_or_size":{"type":"boolean","default":false,"help":"Whether min_size_group and max_size_group refer to the number of groups or their size.","command":"-N"},"min_size_group":{"type":"integer","default":3,"help":"Minimum size of a group of attributes: if numberOfGroups is true, the minimum number of groups.","min":1,"max":10000000000000000,"command":"-G"},"max_size_group":{"type":"integer","default":3,"help":"Maximum size of a group of attributes: if numberOfGroups is true, the maximum number of groups.","min":1,"max":10000000000000000,"command":"-H"},"perc_remove":{"type":"integer","default":50,"help":"Percentage of instances to be removed.","min":1,"max":100,"command":"-P"},"num_iter":{"type":"integer","default":10,"help":" Number of iterations.","min":1,"max":10000000000000000,"command":"-I"},"debug_mode":{"type":"boolean","default":false,"help":"If set, classifier is run in debug mode and may output additional info to the console.","command":"-D"},"seed":{"type":"integer","default":null,"help":"Random number seed.","min":-2147483647,"max":2147483647,"command":"-S"},"base_estimator":{"type":"ensemble","default":"weka.classifiers.trees.RandomTree","help":"Full name of base classifier. (default: weka.classifiers.trees.RandomTree)","command":"-W"}}','weka');
INSERT INTO "algorithms" VALUES (8,'sklearn.tree.DecisionTreeClassifier','Decision Tree','Classification','{"criterion":{"type":"string","default":"gini","help":"The function to measure the quality of a split. Supported criteria are “mse” for the mean squared error, which is equal to variance reduction as feature selection criterion and minimizes the L2 loss using the mean of each terminal node, “friedman_mse”, which uses mean squared error with Friedman’s improvement score for potential splits, and “mae” for the mean absolute error, which minimizes the L1 loss using the median of each terminal node.","options":["gini","entropy"]},"splitter":{"type":"string","default":"best","help":"The strategy used to choose the split at each node. Supported strategies are “best” to choose the best split and “random” to choose the best random split.","options":["best","random"]},"max_depth":{"type":"integer","default":null,"help":"The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.","min":1},"min_samples_split":{"type":"integer","default":2,"help":"The minimum number of samples required to split an internal node.","min":1},"min_samples_leaf":{"type":"integer","default":"1","help":"The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.","min":1},"min_weight_fraction_leaf":{"type":"float","default":0,"help":"The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.","min":0,"max":10000000000000000},"max_features":{"type":"string","default":"auto","help":"The number of features to consider when looking for the best split: If “auto”, then max_features=sqrt(n_features). If “sqrt”, then max_features=sqrt(n_features). If “log2”, then max_features=log2(n_features). If None, then max_features=n_features.","options":["auto","sqrt","log2"]},"max_leaf_nodes":{"type":"integer","default":null,"help":"Grow a tree with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.","min":1},"min_impurity_decrease":{"type":"float","default":0,"help":"A node will be split if this split induces a decrease of the impurity greater than or equal to this value.","min":0,"max":10000000000000000},"min_impurity_split":{"type":"float","default":1e-7,"help":"Threshold for early stopping in tree growth. A node will split if its impurity is above the threshold, otherwise it is a leaf.","min":1e-16,"max":10000000000000000},"presort":{"type":"boolean","default":false,"help":"Whether to presort the data to speed up the finding of best splits in fitting. For the default settings of a decision tree on large datasets, setting this to true may slow down the training process. When using either a smaller dataset or a restricted depth, this may speed up the training."},"random_state":{"type":"integer","default":null,"help":" the seed used by the random number generator.","min":0,"max":4294967295}}','sklearn');
INSERT INTO "algorithms" VALUES (9,'sklearn.tree.DecisionTreeRegressor','Decision Tree','Regression','{"criterion":{"type":"string","default":"mse","help":"The function to measure the quality of a split. Supported criteria are “mse” for the mean squared error, which is equal to variance reduction as feature selection criterion and minimizes the L2 loss using the mean of each terminal node, “friedman_mse”, which uses mean squared error with Friedman’s improvement score for potential splits, and “mae” for the mean absolute error, which minimizes the L1 loss using the median of each terminal node.","options":["mse","friedman_mse","mae"]},"splitter":{"type":"string","default":"best","help":"The strategy used to choose the split at each node. Supported strategies are “best” to choose the best split and “random” to choose the best random split.","options":["best","random"]},"max_depth":{"type":"integer","default":null,"help":"The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.","min":1},"min_samples_split":{"type":"integer","default":2,"help":"The minimum number of samples required to split an internal node.","min":1},"min_samples_leaf":{"type":"integer","default":"1","help":"The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.","min":1},"min_weight_fraction_leaf":{"type":"float","default":0,"help":"The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.","min":0,"max":10000000000000000},"max_features":{"type":"string","default":"auto","help":"The number of features to consider when looking for the best split: If “auto”, then max_features=sqrt(n_features). If “sqrt”, then max_features=sqrt(n_features). If “log2”, then max_features=log2(n_features). If None, then max_features=n_features.","options":["auto","sqrt","log2"]},"max_leaf_nodes":{"type":"integer","default":null,"help":"Grow a tree with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.","min":1},"min_impurity_decrease":{"type":"float","default":0,"help":"A node will be split if this split induces a decrease of the impurity greater than or equal to this value.","min":0,"max":10000000000000000},"presort":{"type":"boolean","default":false,"help":"Whether to presort the data to speed up the finding of best splits in fitting. For the default settings of a decision tree on large datasets, setting this to true may slow down the training process. When using either a smaller dataset or a restricted depth, this may speed up the training."},"random_state":{"type":"integer","default":null,"help":" the seed used by the random number generator.","min":0,"max":4294967295}}','sklearn');
INSERT INTO "algorithms" VALUES (10,'weka.classifiers.functions.LinearRegression','Linear Regression (Weka)','Regression','{"n_selection_method":{"type":"string","default":"0","help":"Set the attribute selection method to use. 1 = None, 2 = Greedy. (default 0 = M5'' method)","options":["0","1","2"],"command":"-S"},"no_try_elim_colinear_attr":{"type":"boolean","defaul":false,"help":"Do not try to eliminate colinear attributes.","command":"-C"},"ridge":{"type":"float","default":1e-8,"help":"Set ridge parameter (default 1.0e-8).","min":1e-16,"max":10000000000000000,"command":"-R"}}','weka');
INSERT INTO "algorithms" VALUES (11,'weka.classifiers.meta.RandomBalance','Random Balance','Classification','{"m_type":{"type":"string","default":"1","help":"functioning mode","options":["1","0","2","3"],"command":"-Y"},"base_estimator":{"type":"ensemble","default":"weka.classifiers.trees.J48","help":"Full name of base classifier. (default: weka.classifiers.trees.J48)","command":"-W"}}','weka');
INSERT INTO "algorithms" VALUES (12,'meka.classifiers.multilabel.BR','Binary Relevance','MultiClassification','{"num-decimal-places":{"type":"integer","default":null,"help":"The number of decimal places for the output of numbers in the model.","min":1,"max":10000000000000000,"command":"-num-decimal-places"},"batch-size":{"type":"integer","default":null,"help":"The desired batch size for batch prediction.","min":1,"max":10000000000000000,"command":"-batch-size"},"base_estimator":{"type":"ensemble","default":"weka.classifiers.trees.J48","help":"Full name of base classifier. (default: weka.classifiers.trees.J48)","command":"-W"}}','meka');
INSERT INTO "algorithms" VALUES (13,'meka.classifiers.multilabel.CC','Classifier Chains','MultiClassification','{"num-decimal-places":{"type":"integer","default":null,"help":"The number of decimal places for the output of numbers in the model.","min":1,"max":10000000000000000,"command":"-num-decimal-places"},"batch-size":{"type":"integer","default":null,"help":"The desired batch size for batch prediction.","min":1,"max":10000000000000000,"command":"-batch-size"},"random_state":{"type":"integer","default":null,"help":"The seed value for randomizing the data. ","min":-2147483647,"max":2147483647,"command":"-S"},"base_estimator":{"type":"ensemble","default":"weka.classifiers.trees.J48","help":"Full name of base classifier. (default: weka.classifiers.trees.J48)","command":"-W"}}','meka');
INSERT INTO "algorithms" VALUES (14,'meka.classifiers.multilabel.LC','Label Powerset','MultiClassification','{"num-decimal-places":{"type":"integer","default":null,"help":"The number of decimal places for the output of numbers in the model.","min":1,"max":10000000000000000,"command":"-num-decimal-places"},"batch-size":{"type":"integer","default":null,"help":"The desired batch size for batch prediction.","min":1,"max":10000000000000000,"command":"-batch-size"},"base_estimator":{"type":"ensemble","default":"weka.classifiers.trees.J48","help":"Full name of base classifier. (default: weka.classifiers.trees.J48)","command":"-W"}}','meka');
INSERT INTO "algorithms" VALUES (15,'meka.classifiers.multilabel.RAkEL','RAkEL','MultiClassification','{"number-subsets":{"type":"integer","default":null,"help":"Sets M: the number of subsets","min":1,"max":10000000000000000,"command":"-M"},"number-labels-subset":{"type":"integer","default":null,"help":"The number of labels k in each subset (must be 0 < k < L for L labels)","min":0,"max":10000000000000000,"command":"-k"},"pruning-value":{"type":"integer","default":null,"help":"Sets the pruning value, defining an infrequent labelset as one which occurs <= P times in the data.","min":1,"max":10000000000000000,"command":"-P"},"max-frequent-labelset":{"type":"integer","default":null,"help":"Sets the (maximum) number of frequent labelsets to subsample from the infrequent labelsets.","min":0,"max":10000000000000000,"command":"-N"},"num-decimal-places":{"type":"integer","default":null,"help":"The number of decimal places for the output of numbers in the model.","min":1,"max":10000000000000000,"command":"-num-decimal-places"},"batch-size":{"type":"integer","default":null,"help":"The desired batch size for batch prediction.","min":1,"max":10000000000000000,"command":"-batch-size"},"random_state":{"type":"integer","default":null,"help":"The seed value for randomizing the data. ","min":-2147483647,"max":2147483647,"command":"-S"},"base_estimator":{"type":"ensemble","default":"weka.classifiers.trees.J48","help":"Full name of base classifier. (default: weka.classifiers.trees.J48)","command":"-W"}}','meka');
INSERT INTO "algorithms" VALUES (19,'weka.classifiers.meta.Bagging','Bagging (Weka)','Mixed','{"bag-size":{"type":"integer","default":null,"help":"Size of each bag, as a percentage of the training set size.","min":1,"max":100,"command":"-P"},"num-execution-slots":{"type":"integer","default":1,"help":"Number of execution slots.","min":1,"max":10000000000000000,"command":"-num-slots"},"num-iterations":{"type":"integer","default":null,"help":"Number of iterations.","min":1,"max":10000000000000000,"command":"-I"},"seed":{"type":"integer","default":null,"help":"Random number seed.","min":1,"max":10000000000000000,"command":"-S"},"base_estimator":{"type":"ensemble","default":"weka.classifiers.trees.REPTree","help":"Full name of base classifier. (default: weka.classifiers.trees.REPTree)","command":"-W"}}','weka');
INSERT INTO "algorithms" VALUES (20,'weka.classifiers.trees.REPTree','REPTree','Mixed','{"min-num-instances-leaf":{"type":"integer","default":2,"help":"Set minimum number of instances per leaf.","min":1,"max":10000000000000000,"command":"-M"},"min-class-variance":{"type":"float","defautl":0.001,"help":"Set minimum numeric class variance proportion of train variance for split","min":1e-16,"max":10000000000000000,"command":"-V"},"num-folds":{"type":"integer","default":3,"help":"Number of folds for reduced error pruning.","min":1,"max":10000000000000000,"command":"-N"},"no-prunnig":{"type":"boolean","default":false,"help":"No pruning.","commmand":"-P"},"seed":{"type":"integer","default":null,"help":"Seed for random data shuffling.","min":1,"max":10000000000000000,"command":"-S"},"max-deep":{"type":"integer","default":null,"help":"Maximum tree depth.","min":1,"max":10000000000000000,"command":"-L"}}','weka');
INSERT INTO "algorithms" VALUES (21,'sklearn.cluster.KMeans','KMeans','Clustering','{"n_clusters":{"type":"integer","default":4,"help":"The number of clusters to form as well as the number of centroids to generate.","min":2,"max":10000000000000000},"init":{"type":"string","default":"k-means++","help":"Method for initialization, defaults to ‘k-means++’: -‘k-means++’ : selects initial cluster centers for k-mean clustering in a smart way to speed up convergence. See section Notes in k_init for more details. -‘random’: choose k observations (rows) at random from data for the initial centroids.","options":["k-means++","random"]},"n_init":{"type":"integer","default":10,"help":"Number of time the k-means algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of inertia.","min":1,"max":10000000000000000},"max_iter":{"type":"integer","default":300,"help":"Maximum number of iterations of the k-means algorithm for a single run.","min":1,"max":10000000000000000},"tol":{"type":"float","default":0.0001,"help":"Relative tolerance with regards to inertia to declare convergence.","min":1e-16,"max":10000000000000000},"precompute_distances":{"type":"boolean","default":true,"help":"True : always precompute distances. False : never precompute distances"},"random_state":{"type":"integer","default":null,"help":"Determines random number generation for centroid initialization.","min":0,"max":10000000000000000},"algorithm":{"type":"string","default":"auto","help":"K-means algorithm to use. The classical EM-style algorithm is “full”. The “elkan” variation is more efficient by using the triangle inequality, but currently doesn’t support sparse data. “auto” chooses “elkan” for dense data and “full” for sparse data.","options":["auto","full","elkan"]}}','sklearn');
INSERT INTO "algorithms" VALUES (22,'sklearn.cluster.AffinityPropagation','AffinityPropagation','Clustering','{"damping":{"type":"float","default":0.5,"help":"Damping factor (between 0.5 and 1) is the extent to which the current value is maintained relative to incoming values (weighted 1 - damping). This in order to avoid numerical oscillations when updating these values (messages).","min":0.5,"max":1},"max_iter":{"type":"integer","default":200,"help":"Maximum number of iterations.","min":1,"max":10000000000000000},"convergence_iter":{"type":"integer","default":15,"help":"Number of iterations with no change in the number of estimated clusters that stops the convergence.","min":1,"max":10000000000000000},"copy":{"type":"boolean","default":true,"help":"Make a copy of input data."},"preference":{"type":"float","default":null,"help":"Preferences for each point - points with larger values of preferences are more likely to be chosen as exemplars. The number of exemplars, ie of clusters, is influenced by the input preferences value. If the preferences are not passed as arguments, they will be set to the median of the input similarities.","min":1,"max":10000000000000000},"affinity":{"type":"string","default":"euclidean","help":"Which affinity to use. At the moment precomputed and euclidean are supported. euclidean uses the negative squared euclidean distance between points.","options":["precomputed","euclidean"]}}','sklearn');
INSERT INTO "algorithms" VALUES (23,'sklearn.cluster.MeanShift','MeanShift','Clustering','{"bandwidth":{"type":"float","default":null,"help":"Bandwidth used in the RBF kernel. If not given, the bandwidth is estimated using sklearn.cluster.estimate_bandwidth; see the documentation for that function for hints on scalability (see also the Notes, below).","min":1,"max":10000000000000000},"bin_seeding":{"type":"boolean","default":false,"help":"If true, initial kernel locations are not locations of all points, but rather the location of the discretized version of points, where points are binned onto a grid whose coarseness corresponds to the bandwidth. Setting this option to True will speed up the algorithm because fewer seeds will be initialized. default value: False"},"min_bin_freq":{"type":"integer","default":1,"help":"To speed up the algorithm, accept only those bins with at least min_bin_freq points as seeds. If not defined, set to 1.","min":1,"max":10000000000000000},"cluster_all":{"type":"boolean","default":true,"help":"If true, then all points are clustered, even those orphans that are not within any kernel. Orphans are assigned to the nearest kernel. If false, then orphans are given cluster label -1."}}','sklearn');
INSERT INTO "algorithms" VALUES (24,'sklearn.neighbors.KNeighborsRegressor','KNN','Regression','{"n_neighbors":{"type":"integer","default":5,"min":1,"help":"Number of neighbors to use by default for kneighbors queries."},"weights":{"type":"string","default":"uniform","help":"Weight function used in prediction. Possible values:\n\n‘uniform’ : uniform weights. All points in each neighborhood are weighted equally.\n‘distance’ : weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away.","options":["uniform","distance"]},"algorithm":{"type":"string","default":"auto","help":"Algorithm used to compute the nearest neighbors:\n‘ball_tree’ will use BallTree\n‘kd_tree’ will use KDTree\n‘brute’ will use a brute-force search.\n‘auto’ will attempt to decide the most appropriate algorithm based on the values passed to fit method.","options":["auto","ball_tree","kd_tree","brute"]},"leaf_size":{"type":"integer","default":30,"help":"Leaf size passed to BallTree or KDTree. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem.","min":1},"p":{"type":"integer","default":2,"help":"Power parameter for the Minkowski metric. When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.","min":1},"metric":{"type":"string","default":"minkowski","help":"The distance metric to use for the tree. The default metric is minkowski, and with p=2 is equivalent to the standard Euclidean metric.","options":["minkowski","euclidean","manhattan","chebyshev","wminkowski","seuclidean","mahalanobis","haversine","hamming","canberra","braycurtis","jaccard","matching","dice","kulsinski","rogerstanimoto","russellrao","sokalmichener","sokalsneath"]}}','sklearn');
INSERT INTO "algorithms" VALUES (25,'sklearn.ensemble.RandomForestClassifier','Random Forest','Classification','{"n_estimators":{"type":"integer","default":100,"min":1,"help":"The number of trees in the forest."},"criterion":{"type":"string","default":"gini","help":"The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain. Note: this parameter is tree-specific.","options":["gini","entropy"]},"max_depth":{"type":"integer","default":null,"help":"The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.","min":1},"min_samples_split":{"type":"integer","default":2,"help":"The minimum number of samples required to split an internal node.","min":2},"min_samples_leaf":{"type":"integer","default":1,"help":"The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.","min":1},"min_weight_fraction_leaf":{"type":"float","default":0e0,"help":"The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided."},"max_features":{"type":"integer","default":null,"help":"The number of features to consider when looking for the best split:\nIf int, then consider max_features features at each split."},"max_leaf_nodes":{"type":"integer","default":null,"help":"Grow trees with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.","min":1},"min_impurity_decrease":{"type":"float","default":0e0,"help":"A node will be split if this split induces a decrease of the impurity greater than or equal to this value."},"bootstrap":{"type":"boolean","default":true,"help":"Whether bootstrap samples are used when building trees. If False, the whole datset is used to build each tree."},"oob_score":{"type":"boolean","default":false,"help":"Whether to use out-of-bag samples to estimate the generalization accuracy."},"random_state":{"type":"integer","default":null,"help":"The seed used by the random number generator.","min":0,"max":4294967295}}','sklearn');
INSERT INTO "algorithms" VALUES (26,'sklearn.ensemble.RandomForestRegressor','Random Forest','Regression','{"n_estimators":{"type":"integer","default":100,"min":1,"help":"The number of trees in the forest."},"criterion":{"type":"string","default":"gini","help":"The function to measure the quality of a split. Supported criteria are “mse” for the mean squared error, which is equal to variance reduction as feature selection criterion, and “mae” for the mean absolute error.","options":["mse","mae"]},"max_depth":{"type":"integer","default":null,"help":"The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.","min":1},"min_samples_split":{"type":"integer","default":2,"help":"The minimum number of samples required to split an internal node.","min":2},"min_samples_leaf":{"type":"integer","default":1,"help":"The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.","min":1},"min_weight_fraction_leaf":{"type":"float","default":0e0,"help":"The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided."},"max_features":{"type":"integer","default":null,"help":"The number of features to consider when looking for the best split:\nIf int, then consider max_features features at each split."},"max_leaf_nodes":{"type":"integer","default":null,"help":"Grow trees with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.","min":1},"min_impurity_decrease":{"type":"float","default":0e0,"help":"A node will be split if this split induces a decrease of the impurity greater than or equal to this value."},"bootstrap":{"type":"boolean","default":true,"help":"Whether bootstrap samples are used when building trees. If False, the whole datset is used to build each tree."},"oob_score":{"type":"boolean","default":false,"help":"whether to use out-of-bag samples to estimate the R^2 on unseen data."},"random_state":{"type":"integer","default":null,"help":"The seed used by the random number generator.","min":0,"max":4294967295}}','sklearn');
INSERT INTO "algorithms" VALUES (27,'sklearn.ensemble.AdaBoostClassifier','AdaBoost','Classification','{"base_estimator":{"type":"ensemble","default":"sklearn.tree.DecisionTreeClassifier","help":"The base estimator from which the boosted ensemble is built."},"n_estimators":{"type":"integer","default":"50","help":"The maximum number of estimators at which boosting is terminated. In case of perfect fit, the learning procedure is stopped early.","min":2},"learning_rate":{"type":"float","default":1e0,"help":"Learning rate shrinks the contribution of each classifier by learning_rate. There is a trade-off between learning_rate and n_estimators.","min":1},"algorithm":{"type":"string","default":"SAMME.R","help":"If ‘SAMME.R’ then use the SAMME.R real boosting algorithm. base_estimator must support calculation of class probabilities. If ‘SAMME’ then use the SAMME discrete boosting algorithm. The SAMME.R algorithm typically converges faster than SAMME, achieving a lower test error with fewer boosting iterations.","options":["SAMME.R","SAMME"]},"random_state":{"type":"integer","default":null,"help":"The seed used by the random number generator.","min":0,"max":4294967295}}','sklearn');
INSERT INTO "algorithms" VALUES (28,'sklearn.ensemble.AdaBoostRegressor','AdaBoost','Regression','{"base_estimator":{"type":"ensemble","default":"sklearn.tree.DecisionTreeRegressor","help":"The base estimator from which the boosted ensemble is built."},"n_estimators":{"type":"integer","default":"50","help":"The maximum number of estimators at which boosting is terminated. In case of perfect fit, the learning procedure is stopped early.","min":2},"learning_rate":{"type":"float","default":1e0,"help":"Learning rate shrinks the contribution of each classifier by learning_rate. There is a trade-off between learning_rate and n_estimators.","min":1},"loss":{"type":"string","default":"linear","help":"The loss function to use when updating the weights after each boosting iteration.","options":["linear","square","exponential"]},"random_state":{"type":"integer","default":null,"help":"The seed used by the random number generator.","min":0,"max":4294967295}}','sklearn');
INSERT INTO "algorithms" VALUES (29,'weka.classifiers.trees.RandomForest','RandomForest (Weka)','Mixed','{"bag_size_percent":{"type":"integer","default":100,"help":"Size of each bag, as a percentage of the training set size.","command":"-P"},"calc_out_of_bag":{"type":"boolean","defaul":false,"help":"Calculate the out of bag error.","command":"-O"},"store_out_of_bag_predictions":{"type":"boolean","default":false,"help":"Whether to store out of bag predictions in internal evaluation object.","command":"-store-out-of-bag-predictions"},"output_out_of_bag_complexity_statistics":{"type":"boolean","default":false,"help":"Whether to output complexity-based statistics when out-of-bag evaluation is performed.","command":"-output-out-of-bag-complexity-statistics"},"compute_attribute_importance":{"type":"boolean","default":false,"help":"Compute and output attribute importance (mean impurity decrease method)","command":"-attribute-importance"},"number_iterations":{"type":"integer","default":100,"help":"Number of iterations.","command":"-I"},"num_execution_slots":{"type":"integer","default":1,"help":"Number of iterations.","command":"-num-slots"},"number_of_attributes":{"type":"integer","default":0,"help":"Number of attributes to randomly investigate. (default 0)\n(<1 = int(log_2(#predictors)+1)).","command":"-K"},"min_number_of_instances":{"type":"integer","default":1,"help":"Set minimum number of instances per leaf. (default 1)","command":"-M"},"min_variance_for_split":{"type":"float","default":1e-3,"help":"  Set minimum numeric class variance proportion of train variance for split (default 1e-3).","command":"-V"},"seed":{"type":"integer","default":1,"help":"Seed for random number generator.","command":"-S"},"depth":{"type":"integer","default":0,"help":"The maximum depth of the tree, 0 for unlimited.(default 0)","command":"-depth"},"num_folds_backfitting":{"type":"integer","default":0,"help":"Number of folds for backfitting (default 0, no backfitting).","command":"-N"},"allow_unclassified":{"type":"boolean","default":false,"help":"Allow unclassified instances.","command":"-U"},"break_ties":{"type":"boolean","default":false,"help":"Break ties randomly when several attributes look equally good.","command":"-B"},"num_decimal_places":{"type":"integer","default":2,"help":"The number of decimal places for the output of numbers in the model (default 2).","command":"-num-decimal-places"},"batch_size":{"type":"integer","default":100,"help":"The desired batch size for batch prediction  (default 100).","command":"-batch-size"}}','weka');
INSERT INTO "algorithms" VALUES (30,'weka.classifiers.meta.AdaBoostM1','AdaBoost M1','Classification','{"weight_percentage":{"type":"integer","default":100,"help":"Percentage of weight mass to base training on.\n(default 100, reduce to around 90 speed up)","command":"-P"},"use_resampling":{"type":"boolean","defaul":false,"help":"Use resampling for boosting.","command":"-Q"},"number_iterations":{"type":"integer","default":10,"help":"Number of iterations.","command":"-I"},"seed":{"type":"integer","default":1,"help":"Seed for random number generator.","command":"-S"},"base_estimator":{"type":"ensemble","default":"weka.classifiers.trees.DecisionStump","help":"Full name of base classifier. (default: weka.classifiers.trees.DecisionStump)","command":"-W"}}','weka');
INSERT INTO "algorithms" VALUES (31,'weka.classifiers.meta.RandomCommittee','Random Committee','Mixed','{"number_iterations":{"type":"integer","default":10,"help":"Number of iterations.","command":"-I"},"seed":{"type":"integer","default":1,"help":"Seed for random number generator.","command":"-S"},"base_estimator":{"type":"ensemble","default":"weka.classifiers.trees.RandomTree","help":"Full name of base classifier. (default: weka.classifiers.trees.RandomTree)","command":"-W"}}','weka');
INSERT INTO "algorithms" VALUES (32,'weka.classifiers.trees.DecisionStump','Decision Stump','Mixed','{}','weka');
INSERT INTO "algorithms" VALUES (33,'weka.classifiers.trees.RandomTree','Random Tree','Mixed','{"number_of_attributes":{"type":"integer","default":0,"help":"Number of attributes to randomly investigate. (default 0)\n(<1 = int(log_2(#predictors)+1)).","command":"-K"},"min_number_of_instances":{"type":"integer","default":1,"help":"Set minimum number of instances per leaf. (default 1)","command":"-M"},"seed":{"type":"integer","default":1,"help":"Seed for random number generator.","command":"-S"},"depth":{"type":"integer","default":0,"help":"The maximum depth of the tree, 0 for unlimited.(default 0)","command":"-depth"},"num_folds_backfitting":{"type":"integer","default":0,"help":"Number of folds for backfitting (default 0, no backfitting).","command":"-N"},"allow_unclassified":{"type":"boolean","default":false,"help":"Allow unclassified instances.","command":"-U"},"break_ties":{"type":"boolean","default":false,"help":"Break ties randomly when several attributes look equally good.","command":"-B"},"num_decimal_places":{"type":"integer","default":2,"help":"The number of decimal places for the output of numbers in the model (default 2).","command":"-num-decimal-places"}}','weka');
INSERT INTO "algorithms" VALUES (34,'weka.classifiers.functions.MultilayerPerceptron','Multilayer Perceptron','Mixed','{"learning_rate":{"type":"float","default":3e-1,"help":"Learning rate for the backpropagation algorithm. (Value should be between 0 - 1).","command":"-L","min":0,"max":1},"momentum":{"type":"float","default":2e-1,"help":"Momentum rate for the backpropagation algorithm.(Value should be between 0 - 1).","command":"-M","min":0,"max":1},"number_of_epochs":{"type":"integer","default":500,"help":"Number of epochs to train through.","min":1,"max":10000000000000000,"command":"-N"},"perc_validation":{"type":"integer","default":0,"help":"Percentage size of validation set to use to terminate training (if this is non zero it can pre-empt num of epochs. (Value should be between 0 - 100).","min":0,"max":100,"command":"-V"},"threshold_consecutive_err":{"type":"integer","default":20,"help":"The number of consecutive increases of error allowed for validation testing before training terminates.","min":1,"max":10000000000000000,"command":"-E"},"not_nominal_binary":{"type":"boolean","default":false,"help":"A NominalToBinary filter will NOT automatically be used. (Set this to not use a NominalToBinary filter).","command":"-B"},"hidden_layer":{"type":"string","default":"a","help":"The hidden layers to be created for the network. (Value should be a list of comma separated Natural numbers or the letters ''a'' = (attribs + classes) / 2, ''i'' = attribs, ''o'' = classes, ''t'' = attribs .+ classes) for wildcard values, Default = a).","command":"-H","options":["a", "i", "o", "t"]},"not_normalize_numeric":{"type":"boolean","default":false,"help":"Normalizing a numeric class will NOT be done. (Set this to not normalize the class if it''s numeric).","command":"-C"},"not_normalize_attrib":{"type":"boolean","default":false,"help":"Normalizing the attributes will NOT be done. (Set this to not normalize the attributes)","command":"-I"},"not_reset_net":{"type":"boolean","default":false,"help":"Reseting the network will NOT be allowed. (Set this to not allow the network to reset).","command":"-R"},"learning_rate_decay":{"type":"boolean","default":false,"help":"Learning rate decay will occur. (Set this to cause the learning rate to decay).","command":"-D"},"seed":{"type":"integer","default":null,"help":"Random number seed.","min":0,"max":2147483647,"command":"-S"}}','weka');
INSERT INTO "algorithms" VALUES (35,'weka.classifiers.bayes.NaiveBayes','Naive Bayes','Classification','{"kernel_density":{"type":"boolean","default":false,"help":"Use kernel density estimator rather than normal distribution for numeric attributes","command":"-K"},"supervised":{"type":"boolean","default":false,"help":"Use supervised discretization to process numeric attributes","command":"-D"}}','weka');
INSERT INTO "algorithms" VALUES (36,'weka.classifiers.functions.Logistic','Logistic Regression (Weka)','Classification','{"ridge":{"type":"float","default":1e-8,"help":"Set the ridge in the log-likelihood.","command":"-R"},"iterations":{"type":"integer","default":-1,"help":"Set the ridge in the log-likelihood.","command":"-M","min":-1}}','weka');
INSERT INTO "algorithms" VALUES (37,'weka.classifiers.meta.LogitBoost','Logit Boost','Classification','{"resampling":{"type":"boolean","default":false,"help":"Use resampling instead of reweighting for boosting.","command":"-Q"},"use_estimated_priors":{"type":"boolean","default":false,"help":"Use estimated priors rather than uniform ones.","command":"-use-estimated-priors"},"percentage":{"type":"float","default":100,"min":0,"max":100,"help":"Percentage of weight mass to base training on. (default 100, reduce to around 90 speed up)","command":"-P"},"threshold":{"type":"float","default":-1.7976931348623157E308,"help":"Threshold on the improvement of the likelihood.","command":"-L"},"shrinkage":{"type":"float","default":1.0,"help":"Shrinkage parameter.","command":"-H"},"z_max_threshold":{"type":"integer","default":null,"help":"Z max threshold for responses.","command":"-Z"},"seed":{"type":"integer","default":1,"help":"Seed for random number generator.","command":"-S"},"number_iterations":{"type":"integer","default":100,"help":"Number of iterations.","command":"-I"},"base_estimator":{"type":"ensemble","default":"weka.classifiers.trees.DecisionStump","help":"Full name of base classifier. (default:weka.classifiers.trees.DecisionStump)","command":"-W"}}','weka');
INSERT INTO "algorithms" VALUES (38,'weka.classifiers.meta.AdditiveRegression','Additive Regression','Regression','{"shrinkage":{"type":"float","default":1.0,"help":"Specify shrinkage rate","command":"-S"},"number_iterations":{"type":"integer","default":10,"help":"Number of iterations.","command":"-I"},"min_absolute_error":{"type":"boolean","default":false,"help":"Minimize absolute error instead of squared error (assumes that base learner minimizes absolute error)."},"base_estimator":{"type":"ensemble","default":"weka.classifiers.trees.DecisionStump","help":"Full name of base classifier. (default:weka.classifiers.trees.DecisionStump)","command":"-W"}}','weka');
INSERT INTO "algorithms" VALUES (39,'weka.classifiers.rules.ZeroR','ZeroR','Mixed','{}','weka');
INSERT INTO "algorithms" VALUES (40,'weka.classifiers.functions.LibSVM','SVM Models','Mixed','{"svm_type":{"type":"integer","default":0,"help":"Set type of SVM:0 = C-SVC, 1 = nu-SVC, 2 = one-class SVM, 3 = epsilon-SVR, 4 = nu-SVR","min":0,"max":4,"command":"-S"},"kernel_type":{"type":"integer","default":2,"help":"Set type of kernel function:0 = linear:u''*v, 1 = polynomial:(gamma*u''*v + coef0)^degree, 2 = radial basis function:exp(-gamma*|u-v|^2), 3 = sigmoid:tanh(gamma*u''*v + coef0)","min":0,"max":3,"command":"-K"},"degree":{"type":"integer","default":3,"help":"Set degree in kernel function","min":1,"command":"-D"},"gamma":{"type":"float","default":0.0,"help":"Set gamma in kernel function","min":0,"command":"-G"},"coef0":{"type":"float","default":0.0,"help":"Set coef0 in kernel function","min":0,"command":"-R"},"cost":{"type":"float","default":1.0,"help":"The parameter C of C-SVC, epsilon-SVR, and nu-SVR","min":0,"command":"-C"},"nu":{"type":"float","default":0.5,"help":"Set the parameter nu of nu-SVC, one-class SVM, and nu-SVR","command":"-N","min":0},"normalize":{"type":"boolean","default":true,"help":"Turns on normalization of input data","command":"-Z"},"no_binary_conversion":{"type":"boolean","default":false,"help":"Turn off nominal to binary conversion. WARNING:use only if your data is all numeric!","command":"-J"},"no_missing_replace":{"type":"boolean","default":false,"help":"Turn off missing value replacement. WARNING:use only if your data has no missing values.","command":"-V"},"loss":{"type":"float","default":0.1,"help":"Set the epsilon in loss function of epsilon-SVR","min":0.1,"command":"-P"},"epsilon":{"type":"float","default":0.001,"help":"Set tolerance of termination criterion","min":1e-10,"max":0.1,"command":"-E"},"no_shrinking":{"type":"boolean","default":true, "help":"Turns the shrinking heuristics off","command":"-H"}}','weka');
INSERT INTO "algorithms" VALUES (41,'weka.classifiers.functions.LibLINEAR','Linear Models','Mixed','{"solver":{"type":"string","default":"1","help":"Set type of solver:0 = L2-regularized logistic regression, 1 = L2-loss support vector machines (dual), 2 = L2-loss support vector machines (primal), 3 = L1-loss support vector machines (dual), 4 = multi-class support vector machines by Crammer and Singer, 5 = L1-regularized L2-loss support vector classification, 6 = L1-regularized logistic regression, 7 = L2-regularized logistic regression (dual), 11 = L2-regularized L2-loss support vector regression (primal), 12 = L2-regularized L2-loss support vector regression (dual), 13 = L2-regularized L1-loss support vector regression (dual)","options":["1", "0", "2", "3", "4", "5", "6", "7", "11", "12", "13"],"command":"-S"},"cost":{"type":"float","default":1.0,"help":"The parameter C","min":0,"command":"-C"},"normalize":{"type":"boolean","default":false,"help":"Turns on normalization of input data","command":"-Z"},"nominal_conversion":{"type":"boolean","default":false,"help":"Turn on nominal to binary conversion.","command":"-N"},"no_missing_replacement":{"type":"boolean","default":false,"help":"Turn off missing value replacement. WARNING:use only if your data has no missing values.","command":"-M"},"probability_estimation":{"type":"boolean","default":false,"help":"Use probability estimation. Currently for L2-regularized logistic regression only!","command":"-P"},"epsilon":{"type":"float","default":0.001,"help":"Set tolerance of termination criterion","min":1e-10,"max":0.1,"command":"-E"},"weight":{"type":"float","default":null,"help":"Set the parameters C of class i to weight[i]*C","min":0,"command":"-W"},"bias":{"type":"float","default":1,"help":"Add Bias term with the given value if >= 0; if < 0, no bias term added","min":0,"command":"-B"}}','weka');
INSERT INTO "algorithms" VALUES (42,'weka.ubu.IteratedBagging','Iterated Bagging','Regression','{"bag_size":{"type":"integer","default":100,"min":0,"max":100,"help":"Size of each bag, as a percentage of the training set size.","command":"-P"},"out_of_bag_error":{"type":"boolean","default":false,"help":"Calculate the out of bag error.","command":"-O"},"num-iterations":{"type":"integer","default":null,"help":"Number of iterations.","min":1,"max":10000000000000000,"command":"-I"},"seed":{"type":"integer","default":null,"help":"Random number seed.","min":1,"max":10000000000000000,"command":"-S"},"base_estimator":{"type":"ensemble","default":"weka.classifiers.trees.REPTree","help":"Full name of base classifier. (default:weka.classifiers.trees.REPTree)","command":"-W"}}','weka');
INSERT INTO "algorithms" VALUES (43,'weka.classifiers.lazy.IBk','IBk','Mixed','{"knn":{"type":"integer","default":3,"min":1,"help":"Number of nearest neighbours (k) used in classification.","command":"-K"},"weight_inverse":{"type":"boolean","default":false,"help":"Weight neighbours by the inverse of their distance (use when k > 1)","command":"-I"},"weight_subtraction":{"type":"boolean","default":false,"help":"Weight neighbours by 1 - their distance (use when k > 1)","command":"-F"}}','weka');
INSERT INTO "algorithms" VALUES (44,'sklearn.cluster.Birch','BIRCH','Clustering','{"threshold":{"type":"float","default":0.5,"help":"The radius of the subcluster obtained by merging a new sample and the closest subcluster should be lesser than the threshold. Otherwise a new subcluster is started. Setting this value to be very low promotes splitting and vice-versa.","min":1e-16,"max":10000000000000000},"branching_factor":{"type":"integer","default":50,"help":"Maximum number of CF subclusters in each node. If a new samples enters such that the number of subclusters exceed the branching_factor then that node is split into two nodes with the subclusters redistributed in each. The parent subcluster of that node is removed and two new subclusters are added as parents of the 2 split nodes.","min":2,"max":10000000000000000},"n_clusters":{"type":"integer","default":3,"help":"Number of clusters after the final clustering step, which treats the subclusters from the leaves as new samples.","min":2,"max":10000000000000000},"compute_labels":{"type":"boolean","default":true,"help":"Whether or not to compute labels for each fit."},"copy":{"type":"boolean","default":true,"help":"Whether or not to make a copy of the given data. If set to False, the initial data will be overwritten."}}','sklearn');
INSERT INTO "algorithms" VALUES (45,'sklearn.mixture.GaussianMixture','Gaussian Mixture','Clustering','{"n_components":{"type":"integer","default":1,"help":"The number of mixture components.","min":1,"max":10000000000000000},"covariance_type":{"type":"string","default":"full","help":"String describing the type of covariance parameters to use. Must be one of: ‘full’ each component has its own general covariance matrix. ‘tied’ all components share the same general covariance matrix. ‘diag’ each component has its own diagonal covariance matrix. ‘spherical’ each component has its own single variance.","options":["full","tied","diag","spherical"]},"tol":{"type":"float","default":0.001,"help":"The convergence threshold. EM iterations will stop when the lower bound average gain is below this threshold.","min":1e-16,"max":10000000000000000},"reg_covar":{"type":"float","default":0.000001,"help":"Non-negative regularization added to the diagonal of covariance. Allows to assure that the covariance matrices are all positive.","min":1e-16,"max":10000000000000000},"max_iter":{"type":"integer","default":100,"help":"The number of EM iterations to perform.","min":1,"max":10000000000000000},"n_init":{"type":"integer","default":1,"help":"The number of initializations to perform. The best results are kept.","min":1,"max":10000000000000000},"init_params":{"type":"string","default":"kmeans","help":"The method used to initialize the weights, the means and the precisions.","options":["kmeans","random"]},"random_state":{"type":"integer","default":null,"help":"Controls the random seed given to the method chosen to initialize the parameters (see init_params). In addition, it controls the generation of random samples from the fitted distribution (see the method sample).","min":0,"max":10000000000000000},"warm_start":{"type":"boolean","default":false,"help":"If ‘warm_start’ is True, the solution of the last fitting is used as initialization for the next call of fit(). This can speed up convergence when fit is called several times on similar problems."}}','sklearn');
INSERT INTO "algorithms" VALUES (46,'weka.classifiers.meta.SphericalOracle','Spherical Oracle','Mixed','{"batchSize":{"type":"integer","default":100,"help":"The preferred number of instances to process if batch prediction is being perfomed.","min":1,"max":1000000000,"command":"-batch-size"},"doNotCheckCapabilities":{"type":"boolean","default":false,"help":"If set, classifer capabilities are not checked before classificer is built (Use with caution to reduce runtime).","command":"-do-not-check-capabilities"},"numDecimalPlaces":{"type":"integer","default":2,"help":"The number of decimal places to be used for the output of numbers in the model.","min":1,"max":1000000000,"command":"-num-decimal-places"},"seed":{"type":"integer","default":null,"help":"The random number seed to be used.","min":1,"max":1000000000,"command":"-S"},"base_estimator":{"type":"ensemble","default":"weka.classifiers.rules.ZeroR","help":"The base classifier to be used.","command":"-W"}}','weka');
INSERT INTO "algorithms" VALUES (47,'weka.classifiers.meta.DisturbingNeighbors','Disturbing Neighbors','Mixed','{"batch-size":{"command":"-batch-size","default":100,"help":"The desired batch size for batch prediction.","max":10000000000000000,"min":1,"type":"integer"},"num-decimal-places":{"command":"-num-decimal-places","default":null,"help":"The number of decimal places for the output of numbers in the model.","max":64,"min":1,"type":"integer"},"useDistances":{"type":"integer","default":false,"help":"Weight neighbours by 1 - their distance (use when k > 1)","command":"-N"},"seed":{"type":"integer","default":null,"help":"Random number seed.","min":1,"max":10000000000000000,"command":"-S"},"base_estimator":{"type":"ensemble","default":"weka.classifiers.rules.ZeroR","help":"Full name of base classifier. (default: weka.classifiers.rules.ZeroR)","command":"-W"}  }','weka');
INSERT INTO "algorithms" VALUES (48,'weka.classifiers.meta.LinearOracle','Linear Oracle','Mixed','{"batch-size":{"command":"-batch-size","default":100,"help":"The desired batch size for batch prediction.","max":10000000000000000,"min":1,"type":"integer"},"num-decimal-places":{"command":"-num-decimal-places","default":null,"help":"The number of decimal places for the output of numbers in the model.","max":64,"min":1,"type":"integer"},"seed":{"type":"integer","default":null,"help":"Random number seed.","min":1,"max":10000000000000000,"command":"-S"},"base_estimator":{"type":"ensemble","default":"weka.classifiers.rules.ZeroR","help":"Full name of base classifier. (default: weka.classifiers.rules.ZeroR)","command":"-W"}}','weka');
INSERT INTO "algorithms" VALUES (50,'sklearn.semi_supervised.SelfTrainingClassifier','Self Training','Classification','{"base_estimator":{"type":"ensemble","default":"sklearn.svm.SVC","help":"The base estimator to fit and predict proba. If None, then the base estimator is an SVC"},"threshold":{"type":"float","default":0.75,"help":"The decision threshold. Should be in [0,1)"},"criterion":{"type":"string","default":"threshold","help":"The selection criterion used to select which labels to add to the training set. If ''threshold'', pseudo-labels with prediction probabilities above threshold are added to the dataset. If ''k_best'', the k_best pseudo-labels with highest prediction probabilities are added to the dataset","options":["threshold","k_best"]},"k_best":{"type":"integer","default":10,"help":"The amount of samples to add in each iteration. Only used when criterion=''k_best''"},"max_iter":{"type":"integer","default":10,"help":"Maximum number of iterations allowed. Should be greater or equal to 0. If it is None, the classifier will continue to predict labels until no new pseudo-labels are added, or all unlabeled samples have been labeled."}}','sklearn');
INSERT INTO "algorithms" VALUES (51,'sklearn.svm.SVC','C-Support Vector','Classification','{"C":{"type":"float","default":1,"help":"Regularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive. The penalty is a squared l2 penalty.","min":0.000001},"kernel":{"type":"string","default":"rbf","options":["rbf","poly","linear","sigmoid","precomputed"],"help":"Specifies the kernel type to be used in the algorithm. If none is given, ‘rbf’ will be used."},"degree":{"type":"integer","default":3,"help":"Degree of the polynomial kernel function (‘poly’). Ignored by all other kernels."},"gamma":{"type":"string","default":"scale","options":["scale","auto"],"help":"Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’."},"coef0":{"type":"float","default":0,"help":"Independent term in kernel function. It is only significant in ‘poly’ and ‘sigmoid’."},"shrinking":{"type":"boolean","default":true,"help":"Whether to use the shrinking heuristic."},"probability":{"type":"boolean","default":true,"help":"Whether to enable probability estimates"},"tol":{"type":"float","default":0.0001,"help":"Tolerance for stopping criterion."},"cache_size":{"type":"float","default":200,"min":1,"help":"Specify the size of the kernel cache (in MB)."},"max_iter":{"type":"intege","default":-1,"help":"Hard limit on iterations within solver, or -1 for no limit."},"random_state":{"type":"integer","default":null,"help":"Controls the pseudo random number generation for shuffling the data for probability estimates. Ignored when probability is False. Pass an int for reproducible output across multiple function calls.","min":0,"max":4294967295}}','sklearn');
INSERT INTO "experiments" VALUES (1,1,'sklearn.neighbors.KNeighborsClassifier','{"n_neighbors":5,"weights":"uniform","algorithm":"auto","p":2}',NULL,NULL,'iris.arff','{"confussion_matrix": [[[15, 0, 0], [0, 14, 0], [0, 0, 16]]], "AUC": [1.0], "f1_score": [1.0], "kappa": [1.0], "accuracy": [1.0]}',1642871383.62766,1642871383.99969,1,'{"mode": "split", "train_partition": 70, "target": ["class"], "columns": ["sepallength", "sepalwidth", "petallength", "petalwidth"], "random_seed": null, "alg_type": "Classification"}');
INSERT INTO "experiments" VALUES (2,1,'sklearn.neighbors.KNeighborsClassifier','{"n_neighbors":3,"weights":"uniform","algorithm":"auto","p":2}',NULL,NULL,'iris.arff','{"confussion_matrix": [[[47, 0, 0], [0, 41, 0], [0, 7, 40]]], "AUC": [0.9627659574468085], "f1_score": [0.9469628481639331], "kappa": [0.9223372781065089], "accuracy": [0.9481481481481482]}',1642871455.12453,1642871455.38686,1,'{"mode": "split", "train_partition": 10, "target": ["class"], "columns": ["petallength", "petalwidth"], "random_seed": null, "alg_type": "Classification"}');
INSERT INTO "experiments" VALUES (4,1,'sklearn.neighbors.KNeighborsClassifier','{"n_neighbors":5,"weights":"uniform","algorithm":"auto","p":2}',NULL,NULL,'iris.arff','{"confussion_matrix": [[[14, 0, 0], [0, 13, 0], [0, 1, 17]]], "AUC": [0.9855324074074074], "f1_score": [0.9781305114638448], "kappa": [0.9664429530201343], "accuracy": [0.9777777777777777]}',1643472796.16639,1643472796.35174,1,'{"mode": "split", "train_partition": 70, "target": ["class"], "columns": ["petallength", "petalwidth"], "random_seed": null, "alg_type": "Classification"}');
INSERT INTO "experiments" VALUES (6,1,'sklearn.semi_supervised.SelfTrainingClassifier','{"base_estimator":{"alg_name":"sklearn.svm.SVC","parameters":{"C":1,"kernel":"rbf","degree":3,"gamma":"scale","coef0":0,"shrinking":true,"probability":false,"tol":0.0001,"cache_size":200,"random_state":null}},"threshold":0.75,"criterion":"threshold","k_best":10,"max_iter":10}',NULL,NULL,'iris.arff','predict_proba is not available when  probability=False',1643474450.25058,1643474450.44403,2,'{"mode": "split", "train_partition": 70, "target": ["class"], "columns": ["petallength", "petalwidth"], "random_seed": null, "alg_type": "Classification"}');
INSERT INTO "experiments" VALUES (7,1,'sklearn.semi_supervised.SelfTrainingClassifier','{"base_estimator":{"alg_name":"sklearn.neighbors.KNeighborsClassifier","parameters":{"n_neighbors":5,"weights":"uniform","algorithm":"auto","p":2}},"threshold":0.75,"criterion":"threshold","k_best":10,"max_iter":10}',NULL,NULL,'iris.arff','{"confussion_matrix": [[[15, 0, 0], [0, 14, 0], [0, 1, 15]]], "AUC": [0.9842069892473119], "f1_score": [0.9777530589543938], "kappa": [0.9666666666666667], "accuracy": [0.9777777777777777]}',1643538021.75878,1643538021.99523,1,'{"mode": "split", "train_partition": 70, "target": ["class"], "columns": ["petallength", "petalwidth"], "random_seed": null, "alg_type": "Classification"}');
INSERT INTO "experiments" VALUES (8,1,'sklearn.semi_supervised.SelfTrainingClassifier','{"base_estimator":{"alg_name":"sklearn.svm.SVC","parameters":{"C":1,"kernel":"rbf","degree":3,"gamma":"scale","coef0":0,"shrinking":true,"probability":true,"tol":0.0001,"cache_size":200,"random_state":null}},"threshold":0.75,"criterion":"threshold","k_best":10,"max_iter":10}',NULL,NULL,'iris.arff','{"confussion_matrix": [[[15, 0, 0], [0, 14, 0], [0, 1, 15]]], "AUC": [0.9842069892473119], "f1_score": [0.9777530589543938], "kappa": [0.9666666666666667], "accuracy": [0.9777777777777777]}',1643539688.5756,1643539688.78653,1,'{"mode": "split", "train_partition": 70, "target": ["class"], "columns": ["petallength", "petalwidth"], "random_seed": null, "alg_type": "Classification"}');
INSERT INTO "experiments" VALUES (9,1,'sklearn.semi_supervised.SelfTrainingClassifier','{"base_estimator":{"alg_name":"sklearn.svm.SVC","parameters":{"C":1,"kernel":"rbf","degree":3,"gamma":"scale","coef0":0,"shrinking":true,"probability":true,"tol":0.0001,"cache_size":200,"random_state":null}},"threshold":0.75,"criterion":"threshold","k_best":10,"max_iter":10}','imblearn.under_sampling.NearMiss','{"sampling_strategy":"auto","varsion":1,"n_neighbors":3,"n_neighbors_ver3":3}','14_banana_norm.arff','__init__() got an unexpected keyword argument ''varsion''',1643539856.17279,1643539856.41873,2,'{"mode": "split", "train_partition": 10, "target": ["Class"], "columns": ["At1", "At2"], "random_seed": null, "alg_type": "Classification"}');
INSERT INTO "experiments" VALUES (10,1,'sklearn.semi_supervised.SelfTrainingClassifier','{"base_estimator":{"alg_name":"sklearn.svm.SVC","parameters":{"C":1,"kernel":"rbf","degree":3,"gamma":"scale","coef0":0,"shrinking":true,"probability":true,"tol":0.0001,"cache_size":200,"random_state":null}},"threshold":0.75,"criterion":"threshold","k_best":10,"max_iter":10}',NULL,NULL,'14_banana_norm.arff','{"confussion_matrix": [[[2454, 170], [354, 1792]]], "ROC": [[[0.0, 0.00038109756097560977, 0.053734756097560975, 0.054496951219512195, 0.09070121951219512, 0.09070121951219512, 0.09832317073170732, 0.09832317073170732, 0.11280487804878049, 0.11280487804878049, 0.11356707317073171, 0.11356707317073171, 0.11471036585365854, 0.11471036585365854, 0.11509146341463415, 0.11509146341463415, 0.11547256097560976, 0.11547256097560976, 0.11585365853658537, 0.11585365853658537, 0.11661585365853659, 0.11661585365853659, 0.11852134146341463, 0.11852134146341463, 0.11890243902439024, 0.11890243902439024, 0.11966463414634146, 0.11966463414634146, 0.12042682926829268, 0.12042682926829268, 0.1211890243902439, 0.1211890243902439, 0.12195121951219512, 0.12195121951219512, 0.12271341463414634, 0.12271341463414634, 0.12347560975609756, 0.12347560975609756, 0.12385670731707317, 0.12385670731707317, 0.12576219512195122, 0.12576219512195122, 0.12690548780487804, 0.12690548780487804, 0.12728658536585366, 0.12728658536585366, 0.1291920731707317, 0.1291920731707317, 0.12957317073170732, 0.12957317073170732, 0.12995426829268292, 0.12995426829268292, 0.13033536585365854, 0.13033536585365854, 0.13185975609756098, 0.13185975609756098, 0.13224085365853658, 0.13224085365853658, 0.13338414634146342, 0.13338414634146342, 0.13452743902439024, 0.13452743902439024, 0.13490853658536586, 0.13490853658536586, 0.13528963414634146, 0.13528963414634146, 0.13567073170731708, 0.13567073170731708, 0.13757621951219512, 0.13757621951219512, 0.13833841463414634, 0.13833841463414634, 0.13871951219512196, 0.13871951219512196, 0.13948170731707318, 0.13948170731707318, 0.13986280487804878, 0.13986280487804878, 0.1410060975609756, 0.1410060975609756, 0.14176829268292682, 0.14176829268292682, 0.14214939024390244, 0.14214939024390244, 0.14253048780487804, 0.14253048780487804, 0.14291158536585366, 0.14291158536585366, 0.14329268292682926, 0.14329268292682926, 0.14367378048780488, 0.14367378048780488, 0.1448170731707317, 0.1448170731707317, 0.14557926829268292, 0.14557926829268292, 0.14710365853658536, 0.14710365853658536, 0.14748475609756098, 0.14748475609756098, 0.14786585365853658, 0.14786585365853658, 0.1486280487804878, 0.1486280487804878, 0.14900914634146342, 0.14900914634146342, 0.14939024390243902, 0.14939024390243902, 0.14977134146341464, 0.14977134146341464, 0.15015243902439024, 0.15015243902439024, 0.15053353658536586, 0.15053353658536586, 0.15129573170731708, 0.15129573170731708, 0.1520579268292683, 0.1520579268292683, 0.15320121951219512, 0.15320121951219512, 0.15510670731707318, 0.15510670731707318, 0.15548780487804878, 0.15548780487804878, 0.15625, 0.15625, 0.15777439024390244, 0.15777439024390244, 0.15891768292682926, 0.15891768292682926, 0.15929878048780488, 0.15929878048780488, 0.15967987804878048, 0.15967987804878048, 0.1600609756097561, 0.1600609756097561, 0.1604420731707317, 0.1604420731707317, 0.16120426829268292, 0.16120426829268292, 0.16158536585365854, 0.16158536585365854, 0.16234756097560976, 0.16234756097560976, 0.16310975609756098, 0.16310975609756098, 0.1638719512195122, 0.1638719512195122, 0.16539634146341464, 0.16539634146341464, 0.16577743902439024, 0.16577743902439024, 0.16615853658536586, 0.16615853658536586, 0.16692073170731708, 0.16692073170731708, 0.16730182926829268, 0.16730182926829268, 0.1680640243902439, 0.1680640243902439, 0.16844512195121952, 0.16844512195121952, 0.16882621951219512, 0.16882621951219512, 0.16920731707317074, 0.16920731707317074, 0.16996951219512196, 0.16996951219512196, 0.17035060975609756, 0.17035060975609756, 0.17073170731707318, 0.17073170731707318, 0.1714939024390244, 0.1714939024390244, 0.1722560975609756, 0.1722560975609756, 0.17339939024390244, 0.17339939024390244, 0.17378048780487804, 0.17378048780487804, 0.17454268292682926, 0.17454268292682926, 0.17530487804878048, 0.17530487804878048, 0.1756859756097561, 0.1756859756097561, 0.1760670731707317, 0.1760670731707317, 0.17721036585365854, 0.17721036585365854, 0.17759146341463414, 0.17759146341463414, 0.17835365853658536, 0.17835365853658536, 0.17911585365853658, 0.17911585365853658, 0.1794969512195122, 0.1794969512195122, 0.1794969512195122, 0.1794969512195122, 0.1798780487804878, 0.1798780487804878, 0.18025914634146342, 0.18025914634146342, 0.18064024390243902, 0.18064024390243902, 0.18140243902439024, 0.18140243902439024, 0.18216463414634146, 0.18216463414634146, 0.18254573170731708, 0.18254573170731708, 0.18292682926829268, 0.18292682926829268, 0.18445121951219512, 0.18445121951219512, 0.18483231707317074, 0.18483231707317074, 0.18521341463414634, 0.18521341463414634, 0.18559451219512196, 0.18559451219512196, 0.18635670731707318, 0.18635670731707318, 0.18673780487804878, 0.18673780487804878, 0.1875, 0.1875, 0.18826219512195122, 0.18826219512195122, 0.18902439024390244, 0.18902439024390244, 0.18940548780487804, 0.18940548780487804, 0.18978658536585366, 0.18978658536585366, 0.19016768292682926, 0.19016768292682926, 0.19054878048780488, 0.19054878048780488, 0.19207317073170732, 0.19207317073170732, 0.19245426829268292, 0.19245426829268292, 0.19321646341463414, 0.19321646341463414, 0.19359756097560976, 0.19359756097560976, 0.1951219512195122, 0.1951219512195122, 0.1955030487804878, 0.1955030487804878, 0.19664634146341464, 0.19664634146341464, 0.19702743902439024, 0.19702743902439024, 0.19778963414634146, 0.19778963414634146, 0.19817073170731708, 0.19817073170731708, 0.1989329268292683, 0.1989329268292683, 0.1993140243902439, 0.1993140243902439, 0.19969512195121952, 0.19969512195121952, 0.20007621951219512, 0.20007621951219512, 0.20045731707317074, 0.20045731707317074, 0.20160060975609756, 0.20160060975609756, 0.2035060975609756, 0.2035060975609756, 0.20388719512195122, 0.20388719512195122, 0.20464939024390244, 0.20464939024390244, 0.20503048780487804, 0.20503048780487804, 0.20541158536585366, 0.20541158536585366, 0.20617378048780488, 0.20617378048780488, 0.20655487804878048, 0.20655487804878048, 0.2069359756097561, 0.2069359756097561, 0.2073170731707317, 0.2073170731707317, 0.20807926829268292, 0.20807926829268292, 0.20884146341463414, 0.20884146341463414, 0.20922256097560976, 0.20922256097560976, 0.20960365853658536, 0.20960365853658536, 0.20998475609756098, 0.20998475609756098, 0.21036585365853658, 0.21036585365853658, 0.2107469512195122, 0.2107469512195122, 0.2111280487804878, 0.2111280487804878, 0.21150914634146342, 0.21150914634146342, 0.21189024390243902, 0.21189024390243902, 0.21379573170731708, 0.21379573170731708, 0.21417682926829268, 0.21417682926829268, 0.2145579268292683, 0.2145579268292683, 0.2149390243902439, 0.2149390243902439, 0.21570121951219512, 0.21570121951219512, 0.21608231707317074, 0.21608231707317074, 0.21646341463414634, 0.21646341463414634, 0.21684451219512196, 0.21684451219512196, 0.21722560975609756, 0.21722560975609756, 0.21760670731707318, 0.21760670731707318, 0.21798780487804878, 0.21798780487804878, 0.2183689024390244, 0.2183689024390244, 0.21875, 0.21875, 0.2191310975609756, 0.2191310975609756, 0.21951219512195122, 0.21951219512195122, 0.21989329268292682, 0.21989329268292682, 0.22027439024390244, 0.22027439024390244, 0.22103658536585366, 0.22103658536585366, 0.22141768292682926, 0.22141768292682926, 0.22179878048780488, 0.22179878048780488, 0.2225609756097561, 0.2225609756097561, 0.2229420731707317, 0.2229420731707317, 0.22332317073170732, 0.22332317073170732, 0.22446646341463414, 0.22446646341463414, 0.22484756097560976, 0.22484756097560976, 0.22522865853658536, 0.22522865853658536, 0.2263719512195122, 0.2263719512195122, 0.2267530487804878, 0.2267530487804878, 0.22713414634146342, 0.22713414634146342, 0.22751524390243902, 0.22751524390243902, 0.22827743902439024, 0.22827743902439024, 0.22865853658536586, 0.22865853658536586, 0.22980182926829268, 0.22980182926829268, 0.2305640243902439, 0.2305640243902439, 0.23094512195121952, 0.23094512195121952, 0.23246951219512196, 0.23246951219512196, 0.23361280487804878, 0.23361280487804878, 0.2347560975609756, 0.2347560975609756, 0.23513719512195122, 0.23513719512195122, 0.23589939024390244, 0.23589939024390244, 0.23704268292682926, 0.23704268292682926, 0.2385670731707317, 0.2385670731707317, 0.23894817073170732, 0.23894817073170732, 0.24009146341463414, 0.24009146341463414, 0.24047256097560976, 0.24047256097560976, 0.24085365853658536, 0.24085365853658536, 0.24161585365853658, 0.24161585365853658, 0.2419969512195122, 0.2419969512195122, 0.24275914634146342, 0.24275914634146342, 0.24314024390243902, 0.24314024390243902, 0.24352134146341464, 0.24352134146341464, 0.24542682926829268, 0.24542682926829268, 0.2458079268292683, 0.2458079268292683, 0.2461890243902439, 0.2461890243902439, 0.24657012195121952, 0.24657012195121952, 0.24695121951219512, 0.24695121951219512, 0.24733231707317074, 0.24733231707317074, 0.24771341463414634, 0.24771341463414634, 0.24809451219512196, 0.24809451219512196, 0.24847560975609756, 0.24847560975609756, 0.24885670731707318, 0.24885670731707318, 0.24923780487804878, 0.24923780487804878, 0.2496189024390244, 0.2496189024390244, 0.25, 0.25, 0.25114329268292684, 0.25114329268292684, 0.25152439024390244, 0.25152439024390244, 0.25228658536585363, 0.25228658536585363, 0.2530487804878049, 0.2530487804878049, 0.2534298780487805, 0.2534298780487805, 0.2538109756097561, 0.2538109756097561, 0.2541920731707317, 0.2541920731707317, 0.2549542682926829, 0.2549542682926829, 0.25609756097560976, 0.25609756097560976, 0.25647865853658536, 0.25647865853658536, 0.25685975609756095, 0.25685975609756095, 0.25952743902439024, 0.25952743902439024, 0.25990853658536583, 0.25990853658536583, 0.2602896341463415, 0.2602896341463415, 0.2621951219512195, 0.2621951219512195, 0.2625762195121951, 0.2625762195121951, 0.2629573170731707, 0.2629573170731707, 0.2648628048780488, 0.2648628048780488, 0.265625, 0.265625, 0.26676829268292684, 0.26676829268292684, 0.26714939024390244, 0.26714939024390244, 0.26753048780487804, 0.26753048780487804, 0.26791158536585363, 0.26791158536585363, 0.2686737804878049, 0.2686737804878049, 0.2701981707317073, 0.2709603658536585, 0.27134146341463417, 0.27134146341463417, 0.2728658536585366, 0.2728658536585366, 0.2736280487804878, 0.2736280487804878, 0.27477134146341464, 0.27477134146341464, 0.27515243902439024, 0.27515243902439024, 0.2774390243902439, 0.2774390243902439, 0.2778201219512195, 0.2778201219512195, 0.2782012195121951, 0.2782012195121951, 0.27934451219512196, 0.27934451219512196, 0.27972560975609756, 0.27972560975609756, 0.28010670731707316, 0.28010670731707316, 0.2804878048780488, 0.2804878048780488, 0.28125, 0.28125, 0.2816310975609756, 0.2816310975609756, 0.28277439024390244, 0.28277439024390244, 0.28315548780487804, 0.28315548780487804, 0.2854420731707317, 0.2854420731707317, 0.2858231707317073, 0.2858231707317073, 0.2862042682926829, 0.2862042682926829, 0.28734756097560976, 0.28734756097560976, 0.28810975609756095, 0.28810975609756095, 0.2884908536585366, 0.2884908536585366, 0.2896341463414634, 0.2896341463414634, 0.29039634146341464, 0.29039634146341464, 0.2919207317073171, 0.2919207317073171, 0.2926829268292683, 0.2926829268292683, 0.2934451219512195, 0.2934451219512195, 0.2938262195121951, 0.2938262195121951, 0.2942073170731707, 0.2942073170731707, 0.29458841463414637, 0.29458841463414637, 0.296875, 0.296875, 0.29801829268292684, 0.29801829268292684, 0.29878048780487804, 0.29878048780487804, 0.2995426829268293, 0.2995426829268293, 0.3003048780487805, 0.3003048780487805, 0.3006859756097561, 0.3006859756097561, 0.3018292682926829, 0.3018292682926829, 0.30259146341463417, 0.30259146341463417, 0.30297256097560976, 0.30297256097560976, 0.30335365853658536, 0.30335365853658536, 0.30373475609756095, 0.30373475609756095, 0.3041158536585366, 0.3041158536585366, 0.30602134146341464, 0.30602134146341464, 0.30640243902439024, 0.30640243902439024, 0.3071646341463415, 0.3083079268292683, 0.3083079268292683, 0.3086890243902439, 0.3086890243902439, 0.31021341463414637, 0.31021341463414637, 0.31059451219512196, 0.31059451219512196, 0.3125, 0.3125, 0.31364329268292684, 0.31364329268292684, 0.31440548780487804, 0.31440548780487804, 0.31478658536585363, 0.31478658536585363, 0.3155487804878049, 0.3155487804878049, 0.3159298780487805, 0.3159298780487805, 0.3163109756097561, 0.3163109756097561, 0.3166920731707317, 0.3166920731707317, 0.3170731707317073, 0.3170731707317073, 0.3178353658536585, 0.3178353658536585, 0.31859756097560976, 0.31859756097560976, 0.3197408536585366, 0.3197408536585366, 0.3201219512195122, 0.3201219512195122, 0.3205030487804878, 0.3205030487804878, 0.3208841463414634, 0.3208841463414634, 0.32126524390243905, 0.32126524390243905, 0.3235518292682927, 0.3235518292682927, 0.3254573170731707, 0.3254573170731707, 0.32583841463414637, 0.32583841463414637, 0.32698170731707316, 0.32698170731707316, 0.3273628048780488, 0.3273628048780488, 0.3277439024390244, 0.3277439024390244, 0.328125, 0.328125, 0.3285060975609756, 0.3285060975609756, 0.32964939024390244, 0.32964939024390244, 0.33003048780487804, 0.33003048780487804, 0.3326981707317073, 0.3326981707317073, 0.3334603658536585, 0.3334603658536585, 0.33384146341463417, 0.33384146341463417, 0.33498475609756095, 0.33498475609756095, 0.3353658536585366, 0.3353658536585366, 0.3357469512195122, 0.3357469512195122, 0.33689024390243905, 0.33689024390243905, 0.33727134146341464, 0.33727134146341464, 0.33803353658536583, 0.33803353658536583, 0.3391768292682927, 0.3391768292682927, 0.3395579268292683, 0.3395579268292683, 0.3399390243902439, 0.3399390243902439, 0.3410823170731707, 0.3410823170731707, 0.34222560975609756, 0.34222560975609756, 0.34375, 0.34375, 0.3441310975609756, 0.3441310975609756, 0.3445121951219512, 0.3445121951219512, 0.34489329268292684, 0.34489329268292684, 0.34565548780487804, 0.34565548780487804, 0.3464176829268293, 0.3464176829268293, 0.3475609756097561, 0.3475609756097561, 0.3479420731707317, 0.3479420731707317, 0.3490853658536585, 0.3490853658536585, 0.35060975609756095, 0.35060975609756095, 0.3513719512195122, 0.3513719512195122, 0.3517530487804878, 0.3517530487804878, 0.3521341463414634, 0.3521341463414634, 0.3544207317073171, 0.3544207317073171, 0.3551829268292683, 0.3551829268292683, 0.3555640243902439, 0.3555640243902439, 0.3559451219512195, 0.3559451219512195, 0.3563262195121951, 0.3563262195121951, 0.36166158536585363, 0.36166158536585363, 0.3624237804878049, 0.3624237804878049, 0.3628048780487805, 0.3628048780487805, 0.3631859756097561, 0.3631859756097561, 0.3635670731707317, 0.3635670731707317, 0.3639481707317073, 0.3639481707317073, 0.3643292682926829, 0.3643292682926829, 0.3647103658536585, 0.3647103658536585, 0.36509146341463417, 0.36509146341463417, 0.36585365853658536, 0.36585365853658536, 0.36623475609756095, 0.36623475609756095, 0.36814024390243905, 0.36814024390243905, 0.36890243902439024, 0.36890243902439024, 0.36928353658536583, 0.36928353658536583, 0.3696646341463415, 0.3696646341463415, 0.3700457317073171, 0.3700457317073171, 0.3708079268292683, 0.3708079268292683, 0.3719512195121951, 0.3719512195121951, 0.3742378048780488, 0.3742378048780488, 0.3757621951219512, 0.3757621951219512, 0.3776676829268293, 0.3776676829268293, 0.3780487804878049, 0.3780487804878049, 0.3799542682926829, 0.3799542682926829, 0.38109756097560976, 0.38109756097560976, 0.38185975609756095, 0.38185975609756095, 0.3822408536585366, 0.3822408536585366, 0.3826219512195122, 0.3826219512195122, 0.38414634146341464, 0.38414634146341464, 0.3860518292682927, 0.3860518292682927, 0.3868140243902439, 0.3868140243902439, 0.3871951219512195, 0.3871951219512195, 0.3875762195121951, 0.3875762195121951, 0.38833841463414637, 0.38833841463414637, 0.38833841463414637, 0.38910060975609756, 0.38910060975609756, 0.38948170731707316, 0.38948170731707316, 0.3902439024390244, 0.3902439024390244, 0.390625, 0.390625, 0.3910060975609756, 0.3910060975609756, 0.39214939024390244, 0.39214939024390244, 0.3932926829268293, 0.3932926829268293, 0.3944359756097561, 0.3944359756097561, 0.3948170731707317, 0.3948170731707317, 0.3951981707317073, 0.3951981707317073, 0.3955792682926829, 0.3955792682926829, 0.39748475609756095, 0.39748475609756095, 0.3978658536585366, 0.3978658536585366, 0.3986280487804878, 0.3986280487804878, 0.3990091463414634, 0.3990091463414634, 0.39977134146341464, 0.39977134146341464, 0.4012957317073171, 0.4012957317073171, 0.4024390243902439, 0.4024390243902439, 0.4032012195121951, 0.4032012195121951, 0.4035823170731707, 0.4035823170731707, 0.40396341463414637, 0.40396341463414637, 0.40510670731707316, 0.40510670731707316, 0.4054878048780488, 0.4054878048780488, 0.4058689024390244, 0.4058689024390244, 0.4066310975609756, 0.4066310975609756, 0.40853658536585363, 0.40853658536585363, 0.4089176829268293, 0.4089176829268293, 0.4092987804878049, 0.4092987804878049, 0.4100609756097561, 0.4100609756097561, 0.4104420731707317, 0.4104420731707317, 0.4115853658536585, 0.4115853658536585, 0.41196646341463417, 0.41196646341463417, 0.41272865853658536, 0.41272865853658536, 0.4138719512195122, 0.4138719512195122, 0.4146341463414634, 0.4146341463414634, 0.41501524390243905, 0.41501524390243905, 0.41539634146341464, 0.41539634146341464, 0.41577743902439024, 0.41577743902439024, 0.41615853658536583, 0.41615853658536583, 0.4169207317073171, 0.4169207317073171, 0.4173018292682927, 0.4173018292682927, 0.4188262195121951, 0.4188262195121951, 0.4192073170731707, 0.4192073170731707, 0.42035060975609756, 0.42035060975609756, 0.4214939024390244, 0.4214939024390244, 0.4245426829268293, 0.4245426829268293, 0.4256859756097561, 0.4256859756097561, 0.4264481707317073, 0.4264481707317073, 0.4272103658536585, 0.4272103658536585, 0.42835365853658536, 0.42835365853658536, 0.4298780487804878, 0.4298780487804878, 0.43102134146341464, 0.43102134146341464, 0.43178353658536583, 0.43178353658536583, 0.4321646341463415, 0.4321646341463415, 0.4336890243902439, 0.4336890243902439, 0.4340701219512195, 0.4340701219512195, 0.4344512195121951, 0.4344512195121951, 0.43559451219512196, 0.43559451219512196, 0.43635670731707316, 0.43635670731707316, 0.4375, 0.4375, 0.4382621951219512, 0.4382621951219512, 0.43864329268292684, 0.43864329268292684, 0.43940548780487804, 0.43940548780487804, 0.43978658536585363, 0.43978658536585363, 0.4405487804878049, 0.4405487804878049, 0.4420731707317073, 0.4420731707317073, 0.4428353658536585, 0.4428353658536585, 0.44321646341463417, 0.44321646341463417, 0.44359756097560976, 0.44359756097560976, 0.4447408536585366, 0.4447408536585366, 0.4455030487804878, 0.4455030487804878, 0.4458841463414634, 0.4458841463414634, 0.44740853658536583, 0.44740853658536583, 0.4477896341463415, 0.4477896341463415, 0.4481707317073171, 0.4481707317073171, 0.4489329268292683, 0.4489329268292683, 0.4496951219512195, 0.4496951219512195, 0.45121951219512196, 0.45121951219512196, 0.45160060975609756, 0.45160060975609756, 0.453125, 0.453125, 0.4538871951219512, 0.4538871951219512, 0.45503048780487804, 0.45503048780487804, 0.45541158536585363, 0.45541158536585363, 0.4557926829268293, 0.4557926829268293, 0.4573170731707317, 0.4573170731707317, 0.4584603658536585, 0.4584603658536585, 0.45884146341463417, 0.45884146341463417, 0.45922256097560976, 0.45922256097560976, 0.45998475609756095, 0.45998475609756095, 0.46265243902439024, 0.46265243902439024, 0.46303353658536583, 0.46303353658536583, 0.4645579268292683, 0.4645579268292683, 0.4653201219512195, 0.4653201219512195, 0.4657012195121951, 0.4657012195121951, 0.46684451219512196, 0.46684451219512196, 0.46722560975609756, 0.46722560975609756, 0.4691310975609756, 0.4691310975609756, 0.4695121951219512, 0.4695121951219512, 0.47027439024390244, 0.47027439024390244, 0.47103658536585363, 0.47103658536585363, 0.4714176829268293, 0.4714176829268293, 0.4725609756097561, 0.4725609756097561, 0.4729420731707317, 0.4729420731707317, 0.47484756097560976, 0.47484756097560976, 0.47522865853658536, 0.47522865853658536, 0.4759908536585366, 0.4759908536585366, 0.4771341463414634, 0.4771341463414634, 0.47789634146341464, 0.47789634146341464, 0.47827743902439024, 0.47827743902439024, 0.47865853658536583, 0.47865853658536583, 0.4790396341463415, 0.4790396341463415, 0.4798018292682927, 0.4798018292682927, 0.4805640243902439, 0.4805640243902439, 0.48208841463414637, 0.48208841463414637, 0.48246951219512196, 0.48246951219512196, 0.4836128048780488, 0.4836128048780488, 0.484375, 0.484375, 0.4851371951219512, 0.4851371951219512, 0.48551829268292684, 0.48551829268292684, 0.4881859756097561, 0.4881859756097561, 0.4885670731707317, 0.4885670731707317, 0.4889481707317073, 0.4889481707317073, 0.4923780487804878, 0.4923780487804878, 0.4950457317073171, 0.4950457317073171, 0.4969512195121951, 0.4969512195121951, 0.49771341463414637, 0.49771341463414637, 0.49847560975609756, 0.49847560975609756, 0.4996189024390244, 0.4996189024390244, 0.5030487804878049, 0.5030487804878049, 0.5034298780487805, 0.5034298780487805, 0.5041920731707317, 0.5041920731707317, 0.506859756097561, 0.506859756097561, 0.5072408536585366, 0.5072408536585366, 0.5091463414634146, 0.5091463414634146, 0.5095274390243902, 0.5095274390243902, 0.5099085365853658, 0.5099085365853658, 0.510670731707317, 0.510670731707317, 0.5110518292682927, 0.5110518292682927, 0.5114329268292683, 0.5114329268292683, 0.5121951219512195, 0.5121951219512195, 0.5129573170731707, 0.5129573170731707, 0.5133384146341463, 0.5133384146341463, 0.5137195121951219, 0.5137195121951219, 0.5141006097560976, 0.5141006097560976, 0.5144817073170732, 0.5144817073170732, 0.5148628048780488, 0.5148628048780488, 0.5163871951219512, 0.5163871951219512, 0.5167682926829268, 0.5167682926829268, 0.5186737804878049, 0.5186737804878049, 0.5194359756097561, 0.5194359756097561, 0.5209603658536586, 0.5209603658536586, 0.5213414634146342, 0.5213414634146342, 0.5217225609756098, 0.5217225609756098, 0.5221036585365854, 0.5221036585365854, 0.522484756097561, 0.522484756097561, 0.5228658536585366, 0.5228658536585366, 0.5236280487804879, 0.5236280487804879, 0.5240091463414634, 0.5240091463414634, 0.5274390243902439, 0.5274390243902439, 0.5282012195121951, 0.5282012195121951, 0.5289634146341463, 0.5289634146341463, 0.5297256097560976, 0.5297256097560976, 0.53125, 0.53125, 0.5320121951219512, 0.5320121951219512, 0.5331554878048781, 0.5331554878048781, 0.5346798780487805, 0.5346798780487805, 0.5350609756097561, 0.5350609756097561, 0.538109756097561, 0.538109756097561, 0.5388719512195121, 0.5388719512195121, 0.540015243902439, 0.540015243902439, 0.5403963414634146, 0.5403963414634146, 0.5407774390243902, 0.5407774390243902, 0.541920731707317, 0.541920731707317, 0.5430640243902439, 0.5430640243902439, 0.5461128048780488, 0.5461128048780488, 0.5487804878048781, 0.5487804878048781, 0.5491615853658537, 0.5491615853658537, 0.5495426829268293, 0.5495426829268293, 0.5503048780487805, 0.5503048780487805, 0.5506859756097561, 0.5506859756097561, 0.5525914634146342, 0.5525914634146342, 0.5544969512195121, 0.5544969512195121, 0.5552591463414634, 0.5552591463414634, 0.555640243902439, 0.555640243902439, 0.5560213414634146, 0.5560213414634146, 0.5564024390243902, 0.5564024390243902, 0.5571646341463414, 0.5571646341463414, 0.557545731707317, 0.557545731707317, 0.5590701219512195, 0.5590701219512195, 0.5602134146341463, 0.5602134146341463, 0.5609756097560976, 0.5609756097560976, 0.5613567073170732, 0.5613567073170732, 0.5621189024390244, 0.5621189024390244, 0.5625, 0.5625, 0.5632621951219512, 0.5632621951219512, 0.5644054878048781, 0.5644054878048781, 0.5651676829268293, 0.5651676829268293, 0.5655487804878049, 0.5655487804878049, 0.5666920731707317, 0.5666920731707317, 0.569359756097561, 0.569359756097561, 0.5705030487804879, 0.5705030487804879, 0.5720274390243902, 0.5720274390243902, 0.5735518292682927, 0.5735518292682927, 0.5739329268292683, 0.5739329268292683, 0.5746951219512195, 0.5746951219512195, 0.5750762195121951, 0.5750762195121951, 0.5758384146341463, 0.5758384146341463, 0.5766006097560976, 0.5766006097560976, 0.5773628048780488, 0.5773628048780488, 0.5788871951219512, 0.5788871951219512, 0.5800304878048781, 0.5800304878048781, 0.5807926829268293, 0.5807926829268293, 0.5811737804878049, 0.5811737804878049, 0.5815548780487805, 0.5815548780487805, 0.5819359756097561, 0.5819359756097561, 0.5826981707317073, 0.5826981707317073, 0.5853658536585366, 0.5853658536585366, 0.5861280487804879, 0.5861280487804879, 0.5872713414634146, 0.5872713414634146, 0.5880335365853658, 0.5880335365853658, 0.5884146341463414, 0.5884146341463414, 0.5891768292682927, 0.5891768292682927, 0.5895579268292683, 0.5895579268292683, 0.5929878048780488, 0.5929878048780488, 0.5964176829268293, 0.5964176829268293, 0.5983231707317073, 0.5990853658536586, 0.5998475609756098, 0.5998475609756098, 0.6009908536585366, 0.6009908536585366, 0.6021341463414634, 0.6021341463414634, 0.6028963414634146, 0.6028963414634146, 0.6048018292682927, 0.6048018292682927, 0.6051829268292683, 0.6051829268292683, 0.6063262195121951, 0.6063262195121951, 0.6070884146341463, 0.6070884146341463, 0.6074695121951219, 0.6074695121951219, 0.609375, 0.609375, 0.6105182926829268, 0.6105182926829268, 0.6108993902439024, 0.6108993902439024, 0.6120426829268293, 0.6120426829268293, 0.6124237804878049, 0.6124237804878049, 0.6131859756097561, 0.6131859756097561, 0.6147103658536586, 0.6147103658536586, 0.616234756097561, 0.616234756097561, 0.6166158536585366, 0.6166158536585366, 0.6169969512195121, 0.6169969512195121, 0.6177591463414634, 0.6177591463414634, 0.6185213414634146, 0.6185213414634146, 0.6192835365853658, 0.6192835365853658, 0.6196646341463414, 0.6196646341463414, 0.620045731707317, 0.620045731707317, 0.6204268292682927, 0.6204268292682927, 0.6215701219512195, 0.6215701219512195, 0.6219512195121951, 0.6219512195121951, 0.6223323170731707, 0.6223323170731707, 0.6242378048780488, 0.6242378048780488, 0.6257621951219512, 0.6257621951219512, 0.6265243902439024, 0.6265243902439024, 0.6291920731707317, 0.6291920731707317, 0.6295731707317073, 0.6295731707317073, 0.6303353658536586, 0.6303353658536586, 0.6310975609756098, 0.6310975609756098, 0.631859756097561, 0.631859756097561, 0.6322408536585366, 0.6322408536585366, 0.6326219512195121, 0.6326219512195121, 0.6333841463414634, 0.6333841463414634, 0.633765243902439, 0.633765243902439, 0.6341463414634146, 0.6341463414634146, 0.6345274390243902, 0.6345274390243902, 0.6349085365853658, 0.6349085365853658, 0.6352896341463414, 0.6352896341463414, 0.6360518292682927, 0.6360518292682927, 0.6364329268292683, 0.6364329268292683, 0.6368140243902439, 0.6368140243902439, 0.6371951219512195, 0.6371951219512195, 0.6375762195121951, 0.6375762195121951, 0.6383384146341463, 0.6383384146341463, 0.6387195121951219, 0.6387195121951219, 0.6391006097560976, 0.6391006097560976, 0.6394817073170732, 0.6394817073170732, 0.6398628048780488, 0.6398628048780488, 0.6398628048780488, 0.6402439024390244, 0.6402439024390244, 0.6417682926829268, 0.6417682926829268, 0.6421493902439024, 0.6421493902439024, 0.6432926829268293, 0.6432926829268293, 0.6436737804878049, 0.6436737804878049, 0.6440548780487805, 0.6440548780487805, 0.6444359756097561, 0.6444359756097561, 0.6451981707317073, 0.6451981707317073, 0.645579268292683, 0.645579268292683, 0.6463414634146342, 0.6463414634146342, 0.6467225609756098, 0.6467225609756098, 0.647484756097561, 0.647484756097561, 0.6490091463414634, 0.6490091463414634, 0.6497713414634146, 0.6497713414634146, 0.6505335365853658, 0.6505335365853658, 0.6520579268292683, 0.6520579268292683, 0.6528201219512195, 0.6528201219512195, 0.6547256097560976, 0.6547256097560976, 0.6551067073170732, 0.6551067073170732, 0.6558689024390244, 0.6558689024390244, 0.65625, 0.65625, 0.6566310975609756, 0.6566310975609756, 0.6570121951219512, 0.6570121951219512, 0.6585365853658537, 0.6585365853658537, 0.6592987804878049, 0.6592987804878049, 0.6604420731707317, 0.6604420731707317, 0.661204268292683, 0.661204268292683, 0.6615853658536586, 0.6615853658536586, 0.6623475609756098, 0.6623475609756098, 0.6627286585365854, 0.6627286585365854, 0.663109756097561, 0.663109756097561, 0.6638719512195121, 0.6638719512195121, 0.665015243902439, 0.665015243902439, 0.6657774390243902, 0.6657774390243902, 0.6661585365853658, 0.6661585365853658, 0.666920731707317, 0.666920731707317, 0.6680640243902439, 0.6680640243902439, 0.6684451219512195, 0.6684451219512195, 0.6692073170731707, 0.6692073170731707, 0.6711128048780488, 0.6711128048780488, 0.671875, 0.671875, 0.6726371951219512, 0.6726371951219512, 0.6737804878048781, 0.6737804878048781, 0.676829268292683, 0.676829268292683, 0.678734756097561, 0.678734756097561, 0.6791158536585366, 0.6791158536585366, 0.6794969512195121, 0.6794969512195121, 0.6798780487804879, 0.6798780487804879, 0.680640243902439, 0.680640243902439, 0.6817835365853658, 0.6817835365853658, 0.682545731707317, 0.682545731707317, 0.6829268292682927, 0.6829268292682927, 0.6833079268292683, 0.6833079268292683, 0.6836890243902439, 0.6836890243902439, 0.6840701219512195, 0.6840701219512195, 0.6844512195121951, 0.6844512195121951, 0.6848323170731707, 0.6848323170731707, 0.6863567073170732, 0.6863567073170732, 0.6871189024390244, 0.6871189024390244, 0.6878810975609756, 0.6878810975609756, 0.6909298780487805, 0.6909298780487805, 0.6916920731707317, 0.6916920731707317, 0.6920731707317073, 0.6920731707317073, 0.6928353658536586, 0.6928353658536586, 0.6932164634146342, 0.6932164634146342, 0.6935975609756098, 0.6935975609756098, 0.6939786585365854, 0.6939786585365854, 0.6947408536585366, 0.6947408536585366, 0.6951219512195121, 0.6951219512195121, 0.696265243902439, 0.696265243902439, 0.6970274390243902, 0.6970274390243902, 0.6977896341463414, 0.6977896341463414, 0.698170731707317, 0.698170731707317, 0.6985518292682927, 0.6985518292682927, 0.6989329268292683, 0.6989329268292683, 0.7004573170731707, 0.7004573170731707, 0.7012195121951219, 0.7012195121951219, 0.7016006097560976, 0.7016006097560976, 0.7019817073170732, 0.7019817073170732, 0.7027439024390244, 0.7027439024390244, 0.703125, 0.703125, 0.7038871951219512, 0.7038871951219512, 0.7050304878048781, 0.7050304878048781, 0.7057926829268293, 0.7057926829268293, 0.7061737804878049, 0.7061737804878049, 0.7073170731707317, 0.7073170731707317, 0.7084603658536586, 0.7084603658536586, 0.7092225609756098, 0.7092225609756098, 0.7096036585365854, 0.7096036585365854, 0.7107469512195121, 0.7107469512195121, 0.7115091463414634, 0.7115091463414634, 0.7122713414634146, 0.7122713414634146, 0.7126524390243902, 0.7126524390243902, 0.7134146341463414, 0.7134146341463414, 0.7141768292682927, 0.7141768292682927, 0.7153201219512195, 0.7153201219512195, 0.7157012195121951, 0.7157012195121951, 0.7160823170731707, 0.7160823170731707, 0.7164634146341463, 0.7164634146341463, 0.7168445121951219, 0.7168445121951219, 0.7179878048780488, 0.7179878048780488, 0.71875, 0.71875, 0.7206554878048781, 0.7206554878048781, 0.7210365853658537, 0.7210365853658537, 0.7214176829268293, 0.7214176829268293, 0.7221798780487805, 0.7221798780487805, 0.7225609756097561, 0.7225609756097561, 0.7233231707317073, 0.7233231707317073, 0.7248475609756098, 0.7248475609756098, 0.7252286585365854, 0.7252286585365854, 0.7267530487804879, 0.7267530487804879, 0.7271341463414634, 0.7271341463414634, 0.7278963414634146, 0.7278963414634146, 0.7290396341463414, 0.7290396341463414, 0.729420731707317, 0.729420731707317, 0.7320884146341463, 0.7320884146341463, 0.7324695121951219, 0.7324695121951219, 0.7336128048780488, 0.7336128048780488, 0.7339939024390244, 0.7339939024390244, 0.734375, 0.734375, 0.7347560975609756, 0.7347560975609756, 0.7355182926829268, 0.7355182926829268, 0.7362804878048781, 0.7362804878048781, 0.7370426829268293, 0.7370426829268293, 0.7378048780487805, 0.7378048780487805, 0.7385670731707317, 0.7385670731707317, 0.7389481707317073, 0.7389481707317073, 0.7397103658536586, 0.7397103658536586, 0.7419969512195121, 0.7419969512195121, 0.7423780487804879, 0.7423780487804879, 0.743140243902439, 0.743140243902439, 0.7446646341463414, 0.7446646341463414, 0.7454268292682927, 0.7454268292682927, 0.7461890243902439, 0.7461890243902439, 0.7469512195121951, 0.7469512195121951, 0.7473323170731707, 0.7473323170731707, 0.7477134146341463, 0.7477134146341463, 0.7480945121951219, 0.7480945121951219, 0.7492378048780488, 0.7492378048780488, 0.75, 0.75, 0.7522865853658537, 0.7522865853658537, 0.7526676829268293, 0.7526676829268293, 0.7534298780487805, 0.7534298780487805, 0.7541920731707317, 0.7541920731707317, 0.7553353658536586, 0.7553353658536586, 0.7560975609756098, 0.7560975609756098, 0.7572408536585366, 0.7572408536585366, 0.7576219512195121, 0.7576219512195121, 0.758765243902439, 0.758765243902439, 0.7595274390243902, 0.7595274390243902, 0.760670731707317, 0.760670731707317, 0.7610518292682927, 0.7610518292682927, 0.7614329268292683, 0.7614329268292683, 0.7621951219512195, 0.7621951219512195, 0.7625762195121951, 0.7625762195121951, 0.7644817073170732, 0.7644817073170732, 0.7648628048780488, 0.7648628048780488, 0.7652439024390244, 0.7652439024390244, 0.765625, 0.765625, 0.7660060975609756, 0.7660060975609756, 0.7667682926829268, 0.7667682926829268, 0.7671493902439024, 0.7675304878048781, 0.7682926829268293, 0.7682926829268293, 0.7690548780487805, 0.7690548780487805, 0.770579268292683, 0.770579268292683, 0.7713414634146342, 0.7713414634146342, 0.772484756097561, 0.772484756097561, 0.7728658536585366, 0.7728658536585366, 0.7732469512195121, 0.7732469512195121, 0.7736280487804879, 0.7736280487804879, 0.7740091463414634, 0.7740091463414634, 0.7747713414634146, 0.7747713414634146, 0.7751524390243902, 0.7751524390243902, 0.7755335365853658, 0.7755335365853658, 0.7759146341463414, 0.7759146341463414, 0.776295731707317, 0.776295731707317, 0.7774390243902439, 0.7774390243902439, 0.7778201219512195, 0.7778201219512195, 0.7782012195121951, 0.7782012195121951, 0.7785823170731707, 0.7785823170731707, 0.7793445121951219, 0.7793445121951219, 0.78125, 0.78125, 0.7827743902439024, 0.7827743902439024, 0.7839176829268293, 0.7839176829268293, 0.7842987804878049, 0.7842987804878049, 0.7846798780487805, 0.7846798780487805, 0.7858231707317073, 0.7858231707317073, 0.786204268292683, 0.786204268292683, 0.7865853658536586, 0.7865853658536586, 0.7884908536585366, 0.7884908536585366, 0.7907774390243902, 0.7907774390243902, 0.7930640243902439, 0.7930640243902439, 0.7934451219512195, 0.7934451219512195, 0.7953506097560976, 0.7953506097560976, 0.7964939024390244, 0.7964939024390244, 0.7980182926829268, 0.7980182926829268, 0.7987804878048781, 0.7987804878048781, 0.7991615853658537, 0.7991615853658537, 0.7999237804878049, 0.7999237804878049, 0.801829268292683, 0.801829268292683, 0.8041158536585366, 0.8041158536585366, 0.8044969512195121, 0.8044969512195121, 0.8048780487804879, 0.8048780487804879, 0.805640243902439, 0.805640243902439, 0.8083079268292683, 0.8083079268292683, 0.8086890243902439, 0.8086890243902439, 0.8094512195121951, 0.8094512195121951, 0.8102134146341463, 0.8102134146341463, 0.8113567073170732, 0.8113567073170732, 0.8121189024390244, 0.8121189024390244, 0.8125, 0.8125, 0.8147865853658537, 0.8147865853658537, 0.8151676829268293, 0.8151676829268293, 0.8155487804878049, 0.8155487804878049, 0.8159298780487805, 0.8159298780487805, 0.8163109756097561, 0.8163109756097561, 0.8166920731707317, 0.8166920731707317, 0.8170731707317073, 0.8170731707317073, 0.8182164634146342, 0.8182164634146342, 0.819359756097561, 0.819359756097561, 0.8201219512195121, 0.8201219512195121, 0.8208841463414634, 0.8208841463414634, 0.8216463414634146, 0.8216463414634146, 0.8239329268292683, 0.8239329268292683, 0.8246951219512195, 0.8246951219512195, 0.8250762195121951, 0.8250762195121951, 0.8254573170731707, 0.8254573170731707, 0.8262195121951219, 0.8262195121951219, 0.8266006097560976, 0.8266006097560976, 0.8277439024390244, 0.8277439024390244, 0.828125, 0.828125, 0.8285060975609756, 0.8285060975609756, 0.8292682926829268, 0.8292682926829268, 0.8300304878048781, 0.8300304878048781, 0.8307926829268293, 0.8307926829268293, 0.8311737804878049, 0.8311737804878049, 0.8315548780487805, 0.8315548780487805, 0.8319359756097561, 0.8319359756097561, 0.8323170731707317, 0.8323170731707317, 0.833079268292683, 0.833079268292683, 0.8334603658536586, 0.8334603658536586, 0.8346036585365854, 0.8346036585365854, 0.8353658536585366, 0.8353658536585366, 0.8357469512195121, 0.8357469512195121, 0.8361280487804879, 0.8361280487804879, 0.836890243902439, 0.836890243902439, 0.8376524390243902, 0.8376524390243902, 0.8384146341463414, 0.8384146341463414, 0.8391768292682927, 0.8391768292682927, 0.8403201219512195, 0.8403201219512195, 0.8407012195121951, 0.8407012195121951, 0.8410823170731707, 0.8410823170731707, 0.8414634146341463, 0.8414634146341463, 0.8418445121951219, 0.8418445121951219, 0.8422256097560976, 0.8422256097560976, 0.8433689024390244, 0.8433689024390244, 0.8448932926829268, 0.8448932926829268, 0.8456554878048781, 0.8456554878048781, 0.8460365853658537, 0.8460365853658537, 0.8464176829268293, 0.8464176829268293, 0.8467987804878049, 0.8467987804878049, 0.8475609756097561, 0.8475609756097561, 0.8479420731707317, 0.8479420731707317, 0.8483231707317073, 0.8483231707317073, 0.848704268292683, 0.848704268292683, 0.8494664634146342, 0.8494664634146342, 0.8498475609756098, 0.8498475609756098, 0.850609756097561, 0.850609756097561, 0.8513719512195121, 0.8513719512195121, 0.8517530487804879, 0.8517530487804879, 0.8521341463414634, 0.8521341463414634, 0.8536585365853658, 0.8536585365853658, 0.854420731707317, 0.854420731707317, 0.8551829268292683, 0.8551829268292683, 0.8555640243902439, 0.8555640243902439, 0.8559451219512195, 0.8559451219512195, 0.8563262195121951, 0.8563262195121951, 0.8578506097560976, 0.8578506097560976, 0.8586128048780488, 0.8586128048780488, 0.8589939024390244, 0.8589939024390244, 0.8601371951219512, 0.8601371951219512, 0.8612804878048781, 0.8612804878048781, 0.8624237804878049, 0.8624237804878049, 0.864329268292683, 0.864329268292683, 0.8647103658536586, 0.8647103658536586, 0.8658536585365854, 0.8658536585365854, 0.8666158536585366, 0.8666158536585366, 0.8677591463414634, 0.8677591463414634, 0.868140243902439, 0.868140243902439, 0.8696646341463414, 0.8696646341463414, 0.8704268292682927, 0.8704268292682927, 0.8711890243902439, 0.8711890243902439, 0.8715701219512195, 0.8715701219512195, 0.8719512195121951, 0.8719512195121951, 0.8727134146341463, 0.8727134146341463, 0.8730945121951219, 0.8730945121951219, 0.8738567073170732, 0.8738567073170732, 0.8742378048780488, 0.8742378048780488, 0.8757621951219512, 0.8757621951219512, 0.8765243902439024, 0.8765243902439024, 0.8776676829268293, 0.8776676829268293, 0.8788109756097561, 0.8788109756097561, 0.8791920731707317, 0.8791920731707317, 0.8795731707317073, 0.8795731707317073, 0.8803353658536586, 0.8803353658536586, 0.8810975609756098, 0.8810975609756098, 0.881859756097561, 0.881859756097561, 0.8822408536585366, 0.8822408536585366, 0.8826219512195121, 0.8826219512195121, 0.8830030487804879, 0.8830030487804879, 0.883765243902439, 0.883765243902439, 0.8849085365853658, 0.8849085365853658, 0.8871951219512195, 0.8871951219512195, 0.8875762195121951, 0.8875762195121951, 0.8879573170731707, 0.8879573170731707, 0.8883384146341463, 0.8883384146341463, 0.890625, 0.890625, 0.8910060975609756, 0.8910060975609756, 0.8932926829268293, 0.8932926829268293, 0.8959603658536586, 0.8959603658536586, 0.8963414634146342, 0.8963414634146342, 0.8978658536585366, 0.8978658536585366, 0.8982469512195121, 0.8982469512195121, 0.8986280487804879, 0.8986280487804879, 0.9009146341463414, 0.9009146341463414, 0.901295731707317, 0.901295731707317, 0.9028201219512195, 0.9028201219512195, 0.9039634146341463, 0.9039634146341463, 0.9043445121951219, 0.9043445121951219, 0.9047256097560976, 0.9047256097560976, 0.9054878048780488, 0.9054878048780488, 0.9058689024390244, 0.9058689024390244, 0.9070121951219512, 0.9070121951219512, 0.9096798780487805, 0.9096798780487805, 0.911204268292683, 0.911204268292683, 0.9115853658536586, 0.9115853658536586, 0.9123475609756098, 0.9123475609756098, 0.9127286585365854, 0.9127286585365854, 0.9138719512195121, 0.9138719512195121, 0.915015243902439, 0.915015243902439, 0.9157774390243902, 0.9157774390243902, 0.9161585365853658, 0.9161585365853658, 0.9207317073170732, 0.9207317073170732, 0.9222560975609756, 0.9222560975609756, 0.9226371951219512, 0.9226371951219512, 0.9230182926829268, 0.9230182926829268, 0.9237804878048781, 0.9237804878048781, 0.9241615853658537, 0.9241615853658537, 0.9249237804878049, 0.9249237804878049, 0.926829268292683, 0.926829268292683, 0.9279725609756098, 0.9279725609756098, 0.9302591463414634, 0.9302591463414634, 0.930640243902439, 0.930640243902439, 0.9317835365853658, 0.9317835365853658, 0.932545731707317, 0.932545731707317, 0.9329268292682927, 0.9329268292682927, 0.9333079268292683, 0.9333079268292683, 0.9336890243902439, 0.9336890243902439, 0.9359756097560976, 0.9359756097560976, 0.9363567073170732, 0.9363567073170732, 0.9371189024390244, 0.9371189024390244, 0.9375, 0.9375, 0.9386432926829268, 0.9386432926829268, 0.9397865853658537, 0.9397865853658537, 0.9420731707317073, 0.9420731707317073, 0.9428353658536586, 0.9428353658536586, 0.9432164634146342, 0.9432164634146342, 0.9439786585365854, 0.9439786585365854, 0.9447408536585366, 0.9447408536585366, 0.9451219512195121, 0.9451219512195121, 0.9455030487804879, 0.9455030487804879, 0.946265243902439, 0.946265243902439, 0.9470274390243902, 0.9470274390243902, 0.948170731707317, 0.948170731707317, 0.9485518292682927, 0.9485518292682927, 0.9489329268292683, 0.9489329268292683, 0.9493140243902439, 0.9493140243902439, 0.9500762195121951, 0.9500762195121951, 0.9508384146341463, 0.9508384146341463, 0.9512195121951219, 0.9512195121951219, 0.9527439024390244, 0.9527439024390244, 0.9535060975609756, 0.9535060975609756, 0.9554115853658537, 0.9554115853658537, 0.9557926829268293, 0.9557926829268293, 0.9561737804878049, 0.9561737804878049, 0.9569359756097561, 0.9569359756097561, 0.9573170731707317, 0.9573170731707317, 0.9603658536585366, 0.9603658536585366, 0.9622713414634146, 0.9622713414634146, 0.9630335365853658, 0.9630335365853658, 0.9641768292682927, 0.9641768292682927, 0.9649390243902439, 0.9649390243902439, 0.9657012195121951, 0.9657012195121951, 0.9660823170731707, 0.9660823170731707, 0.9672256097560976, 0.9672256097560976, 0.9683689024390244, 0.9683689024390244, 0.9695121951219512, 0.9695121951219512, 0.9706554878048781, 0.9706554878048781, 0.9710365853658537, 0.9710365853658537, 0.9717987804878049, 0.9717987804878049, 0.9725609756097561, 0.9725609756097561, 0.9729420731707317, 0.9729420731707317, 0.9733231707317073, 0.9733231707317073, 0.9744664634146342, 0.9744664634146342, 0.9748475609756098, 0.9748475609756098, 0.9759908536585366, 0.9759908536585366, 0.9786585365853658, 0.9786585365853658, 0.979420731707317, 0.979420731707317, 0.9805640243902439, 0.9805640243902439, 0.9820884146341463, 0.9820884146341463, 0.9824695121951219, 0.9824695121951219, 0.9828506097560976, 0.9828506097560976, 0.9836128048780488, 0.9836128048780488, 0.9839939024390244, 0.9839939024390244, 0.984375, 0.984375, 0.9851371951219512, 0.9851371951219512, 0.9855182926829268, 0.9855182926829268, 0.9862804878048781, 0.9862804878048781, 0.9866615853658537, 0.9866615853658537, 0.9874237804878049, 0.9874237804878049, 0.9881859756097561, 0.9881859756097561, 0.9897103658536586, 0.9897103658536586, 0.9900914634146342, 0.9900914634146342, 0.9908536585365854, 0.9908536585365854, 0.9927591463414634, 0.9927591463414634, 0.993140243902439, 0.993140243902439, 0.9942835365853658, 0.9942835365853658, 0.9946646341463414, 0.9946646341463414, 0.9954268292682927, 0.9954268292682927, 0.9958079268292683, 0.9958079268292683, 0.9973323170731707, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.00046598322460391424, 0.00046598322460391424, 0.0009319664492078285, 0.0009319664492078285, 0.0013979496738117428, 0.0013979496738117428, 0.001863932898415657, 0.001863932898415657, 0.002329916123019571, 0.002329916123019571, 0.0027958993476234857, 0.0027958993476234857, 0.003727865796831314, 0.003727865796831314, 0.005591798695246971, 0.005591798695246971, 0.0065237651444548, 0.0065237651444548, 0.008387698042870456, 0.008387698042870456, 0.009319664492078284, 0.009319664492078284, 0.011183597390493943, 0.011183597390493943, 0.012581547064305684, 0.012581547064305684, 0.01537744641192917, 0.01537744641192917, 0.017707362534948742, 0.017707362534948742, 0.01863932898415657, 0.01863932898415657, 0.019105312208760484, 0.019105312208760484, 0.0195712954333644, 0.0195712954333644, 0.02050326188257223, 0.02050326188257223, 0.02096924510717614, 0.02096924510717614, 0.021435228331780055, 0.021435228331780055, 0.02190121155638397, 0.02190121155638397, 0.022367194780987885, 0.022367194780987885, 0.023299161230195712, 0.023299161230195712, 0.024697110904007457, 0.024697110904007457, 0.02516309412861137, 0.02516309412861137, 0.0260950605778192, 0.0260950605778192, 0.02702702702702703, 0.02702702702702703, 0.028890959925442685, 0.028890959925442685, 0.03075489282385834, 0.03075489282385834, 0.032618825722273995, 0.032618825722273995, 0.033550792171481825, 0.033550792171481825, 0.03401677539608574, 0.03401677539608574, 0.034482758620689655, 0.034482758620689655, 0.036346691519105315, 0.036346691519105315, 0.03681267474370923, 0.03681267474370923, 0.03960857409133271, 0.03960857409133271, 0.04007455731593663, 0.04007455731593663, 0.041472506989748366, 0.041472506989748366, 0.044268406337371856, 0.044268406337371856, 0.047530288909599254, 0.047530288909599254, 0.048462255358807084, 0.048462255358807084, 0.04986020503261883, 0.04986020503261883, 0.0521901211556384, 0.0521901211556384, 0.05265610438024231, 0.05265610438024231, 0.05312208760484623, 0.05312208760484623, 0.05405405405405406, 0.05405405405405406, 0.05498602050326188, 0.05498602050326188, 0.055452003727865795, 0.055452003727865795, 0.05591798695246971, 0.05591798695246971, 0.056383970177073625, 0.056383970177073625, 0.05917986952469711, 0.05917986952469711, 0.05964585274930102, 0.05964585274930102, 0.06057781919850885, 0.06057781919850885, 0.06244175209692451, 0.06244175209692451, 0.06337371854613234, 0.06337371854613234, 0.06383970177073625, 0.06383970177073625, 0.06523765144454799, 0.06523765144454799, 0.06570363466915191, 0.06570363466915191, 0.06663560111835974, 0.06663560111835974, 0.0684995340167754, 0.0684995340167754, 0.07082945013979497, 0.07082945013979497, 0.07129543336439888, 0.07129543336439888, 0.07269338303821063, 0.07269338303821063, 0.07362534948741846, 0.07362534948741846, 0.07735321528424977, 0.07735321528424977, 0.0787511649580615, 0.0787511649580615, 0.07921714818266543, 0.07921714818266543, 0.08014911463187326, 0.08014911463187326, 0.08061509785647716, 0.08061509785647716, 0.08247903075489282, 0.08247903075489282, 0.08434296365330848, 0.08434296365330848, 0.08620689655172414, 0.08620689655172414, 0.08667287977632805, 0.08667287977632805, 0.08900279589934762, 0.08900279589934762, 0.09086672879776328, 0.09086672879776328, 0.09319664492078285, 0.09319664492078285, 0.09412861136999068, 0.09412861136999068, 0.0945945945945946, 0.0945945945945946, 0.09645852749301025, 0.09645852749301025, 0.09692451071761417, 0.09692451071761417, 0.0983224603914259, 0.0983224603914259, 0.10018639328984157, 0.10018639328984157, 0.10065237651444547, 0.10065237651444547, 0.10205032618825723, 0.10205032618825723, 0.10298229263746506, 0.10298229263746506, 0.1043802423112768, 0.1043802423112768, 0.10531220876048462, 0.10531220876048462, 0.10624417520969245, 0.10624417520969245, 0.10671015843429636, 0.10671015843429636, 0.10717614165890028, 0.10717614165890028, 0.10764212488350419, 0.10764212488350419, 0.10810810810810811, 0.10810810810810811, 0.10857409133271202, 0.10857409133271202, 0.10904007455731593, 0.10904007455731593, 0.10950605778191985, 0.10950605778191985, 0.10997204100652376, 0.11090400745573159, 0.11276794035414725, 0.11276794035414725, 0.11509785647716682, 0.11509785647716682, 0.11602982292637465, 0.11602982292637465, 0.11649580615097857, 0.11649580615097857, 0.11742777260018639, 0.11742777260018639, 0.11789375582479031, 0.11789375582479031, 0.11975768872320597, 0.11975768872320597, 0.12022367194780988, 0.12022367194780988, 0.12208760484622554, 0.12208760484622554, 0.12348555452003727, 0.12348555452003727, 0.1239515377446412, 0.1239515377446412, 0.1244175209692451, 0.1244175209692451, 0.12534948741845295, 0.12534948741845295, 0.12628145386766076, 0.12628145386766076, 0.12721342031686858, 0.12721342031686858, 0.1276794035414725, 0.1276794035414725, 0.13187325256290774, 0.13187325256290774, 0.13233923578751164, 0.13233923578751164, 0.1337371854613234, 0.1337371854613234, 0.1342031686859273, 0.1342031686859273, 0.13513513513513514, 0.13513513513513514, 0.1369990680335508, 0.1369990680335508, 0.13793103448275862, 0.13793103448275862, 0.13886300093196646, 0.13886300093196646, 0.1407269338303821, 0.1407269338303821, 0.14212488350419386, 0.14212488350419386, 0.1435228331780056, 0.1435228331780056, 0.14585274930102515, 0.14585274930102515, 0.14725069897483692, 0.14725069897483692, 0.1477166821994408, 0.1477166821994408, 0.1509785647716682, 0.1509785647716682, 0.15191053122087605, 0.15191053122087605, 0.15284249767008387, 0.15284249767008387, 0.15377446411929171, 0.15377446411929171, 0.15517241379310345, 0.15517241379310345, 0.1570363466915191, 0.1570363466915191, 0.157502329916123, 0.157502329916123, 0.15796831314072693, 0.15796831314072693, 0.15936626281453867, 0.15936626281453867, 0.1598322460391426, 0.1598322460391426, 0.16076421248835043, 0.16076421248835043, 0.16216216216216217, 0.16216216216216217, 0.16449207828518173, 0.16449207828518173, 0.16589002795899346, 0.16589002795899346, 0.1668219944082013, 0.1668219944082013, 0.16961789375582478, 0.16961789375582478, 0.1700838769804287, 0.1700838769804287, 0.17148182665424044, 0.17148182665424044, 0.1733457595526561, 0.1733457595526561, 0.17427772600186392, 0.17427772600186392, 0.17474370922646784, 0.17474370922646784, 0.17520969245107176, 0.17520969245107176, 0.17567567567567569, 0.17567567567567569, 0.17614165890027958, 0.17614165890027958, 0.17707362534948742, 0.17707362534948742, 0.18126747437092264, 0.18126747437092264, 0.18406337371854614, 0.18406337371854614, 0.18452935694315004, 0.18452935694315004, 0.18732525629077354, 0.18732525629077354, 0.19012115563839702, 0.19012115563839702, 0.19338303821062441, 0.19338303821062441, 0.19524697110904007, 0.19524697110904007, 0.1961789375582479, 0.1961789375582479, 0.1989748369058714, 0.1989748369058714, 0.20037278657968313, 0.20037278657968313, 0.2031686859273066, 0.2031686859273066, 0.20410065237651445, 0.20410065237651445, 0.206430568499534, 0.206430568499534, 0.20689655172413793, 0.20689655172413793, 0.20829450139794967, 0.20829450139794967, 0.2092264678471575, 0.2092264678471575, 0.2096924510717614, 0.2096924510717614, 0.21109040074557317, 0.21109040074557317, 0.21342031686859272, 0.21342031686859272, 0.21388630009319665, 0.21388630009319665, 0.21528424976700838, 0.21528424976700838, 0.21668219944082012, 0.21668219944082012, 0.21761416589002797, 0.21761416589002797, 0.21808014911463186, 0.21808014911463186, 0.21947809878844363, 0.21947809878844363, 0.21994408201304752, 0.21994408201304752, 0.22041006523765144, 0.22041006523765144, 0.22367194780987884, 0.22367194780987884, 0.22413793103448276, 0.22413793103448276, 0.22460391425908668, 0.22460391425908668, 0.2255358807082945, 0.2255358807082945, 0.22739981360671016, 0.22739981360671016, 0.2287977632805219, 0.2287977632805219, 0.23019571295433364, 0.23019571295433364, 0.2320596458527493, 0.2320596458527493, 0.23299161230195714, 0.23299161230195714, 0.23578751164958062, 0.23578751164958062, 0.2385834109972041, 0.2385834109972041, 0.24370922646784715, 0.24370922646784715, 0.24417520969245107, 0.24417520969245107, 0.2451071761416589, 0.2451071761416589, 0.24603914259086673, 0.24603914259086673, 0.24650512581547065, 0.24650512581547065, 0.24697110904007455, 0.24697110904007455, 0.2479030754892824, 0.2479030754892824, 0.2483690587138863, 0.2483690587138863, 0.2488350419384902, 0.2488350419384902, 0.25023299161230195, 0.25023299161230195, 0.2511649580615098, 0.2511649580615098, 0.2516309412861137, 0.2516309412861137, 0.25209692451071763, 0.25209692451071763, 0.25256290773532153, 0.25256290773532153, 0.2534948741845294, 0.2534948741845294, 0.255358807082945, 0.255358807082945, 0.25629077353215285, 0.25629077353215285, 0.2572227399813607, 0.2572227399813607, 0.25862068965517243, 0.25862068965517243, 0.2595526561043802, 0.2595526561043802, 0.26001863932898417, 0.26001863932898417, 0.26095060577819196, 0.26095060577819196, 0.2614165890027959, 0.2614165890027959, 0.26281453867660765, 0.26281453867660765, 0.2651444547996272, 0.2651444547996272, 0.266076421248835, 0.266076421248835, 0.26654240447343897, 0.26654240447343897, 0.26700838769804286, 0.26700838769804286, 0.26933830382106244, 0.26933830382106244, 0.26980428704566634, 0.26980428704566634, 0.271668219944082, 0.271668219944082, 0.27260018639328987, 0.27260018639328987, 0.27306616961789376, 0.27306616961789376, 0.2739981360671016, 0.2739981360671016, 0.2744641192917055, 0.2744641192917055, 0.2749301025163094, 0.2749301025163094, 0.27539608574091334, 0.27539608574091334, 0.27586206896551724, 0.27586206896551724, 0.2767940354147251, 0.2767940354147251, 0.277260018639329, 0.277260018639329, 0.2777260018639329, 0.2777260018639329, 0.2781919850885368, 0.2781919850885368, 0.2786579683131407, 0.2786579683131407, 0.27912395153774466, 0.27912395153774466, 0.28005591798695245, 0.28005591798695245, 0.2805219012115564, 0.2805219012115564, 0.2809878844361603, 0.2809878844361603, 0.2814538676607642, 0.2814538676607642, 0.28191985088536814, 0.28191985088536814, 0.28238583410997203, 0.28238583410997203, 0.2833178005591799, 0.2833178005591799, 0.2833178005591799, 0.2833178005591799, 0.28378378378378377, 0.28378378378378377, 0.2851817334575955, 0.2851817334575955, 0.28564771668219946, 0.28564771668219946, 0.2870456663560112, 0.2870456663560112, 0.287977632805219, 0.287977632805219, 0.28844361602982294, 0.28844361602982294, 0.2912395153774464, 0.2912395153774464, 0.29217148182665426, 0.29217148182665426, 0.2940354147250699, 0.2940354147250699, 0.29450139794967384, 0.29450139794967384, 0.29496738117427773, 0.29496738117427773, 0.2954333643988816, 0.2954333643988816, 0.29636533084808947, 0.29636533084808947, 0.29683131407269336, 0.29683131407269336, 0.2972972972972973, 0.2972972972972973, 0.2977632805219012, 0.2977632805219012, 0.29869524697110905, 0.29869524697110905, 0.29916123019571295, 0.29916123019571295, 0.3000931966449208, 0.3000931966449208, 0.3005591798695247, 0.3005591798695247, 0.30149114631873253, 0.30149114631873253, 0.30242311276794037, 0.30242311276794037, 0.30335507921714816, 0.30335507921714816, 0.304287045666356, 0.304287045666356, 0.30521901211556385, 0.30521901211556385, 0.30568499534016774, 0.30568499534016774, 0.3066169617893756, 0.3066169617893756, 0.3070829450139795, 0.3070829450139795, 0.30754892823858343, 0.30754892823858343, 0.30987884436160296, 0.30987884436160296, 0.31127679403541475, 0.31127679403541475, 0.3140726933830382, 0.3140726933830382, 0.31547064305684996, 0.31547064305684996, 0.31593662628145386, 0.31593662628145386, 0.3164026095060578, 0.3164026095060578, 0.3168685927306617, 0.3168685927306617, 0.3173345759552656, 0.3173345759552656, 0.31873252562907733, 0.31873252562907733, 0.320596458527493, 0.320596458527493, 0.3210624417520969, 0.3210624417520969, 0.32199440820130476, 0.32199440820130476, 0.3233923578751165, 0.3233923578751165, 0.3238583410997204, 0.3238583410997204, 0.32525629077353213, 0.32525629077353213, 0.32525629077353213, 0.3257222739981361, 0.3257222739981361, 0.32618825722274, 0.32618825722274, 0.32665424044734387, 0.32665424044734387, 0.32805219012115566, 0.32805219012115566, 0.3299161230195713, 0.3299161230195713, 0.3303821062441752, 0.3303821062441752, 0.33084808946877914, 0.33084808946877914, 0.3317800559179869, 0.3317800559179869, 0.3322460391425909, 0.3322460391425909, 0.3331780055917987, 0.3331780055917987, 0.33504193849021435, 0.33504193849021435, 0.33550792171481825, 0.33550792171481825, 0.3364398881640261, 0.3364398881640261, 0.33737185461323393, 0.33737185461323393, 0.3383038210624418, 0.3383038210624418, 0.3387698042870457, 0.3387698042870457, 0.33923578751164957, 0.33923578751164957, 0.3397017707362535, 0.3397017707362535, 0.34203168685927304, 0.34203168685927304, 0.342497670083877, 0.342497670083877, 0.34342963653308484, 0.34342963653308484, 0.34389561975768873, 0.34389561975768873, 0.3448275862068966, 0.3448275862068966, 0.34575955265610436, 0.34575955265610436, 0.34808946877912395, 0.34808946877912395, 0.34855545200372784, 0.34855545200372784, 0.3494874184529357, 0.3494874184529357, 0.34995340167753963, 0.34995340167753963, 0.35135135135135137, 0.35135135135135137, 0.35228331780055916, 0.35228331780055916, 0.353215284249767, 0.353215284249767, 0.3536812674743709, 0.3536812674743709, 0.35414725069897485, 0.35414725069897485, 0.3555452003727866, 0.3555452003727866, 0.35647716682199443, 0.35647716682199443, 0.3569431500465983, 0.3569431500465983, 0.3574091332712022, 0.3574091332712022, 0.35787511649580617, 0.35787511649580617, 0.3592730661696179, 0.3592730661696179, 0.3597390493942218, 0.3597390493942218, 0.36020503261882575, 0.36020503261882575, 0.36067101584342964, 0.36067101584342964, 0.3616029822926375, 0.3616029822926375, 0.3620689655172414, 0.3620689655172414, 0.3625349487418453, 0.3625349487418453, 0.363932898415657, 0.363932898415657, 0.36486486486486486, 0.36486486486486486, 0.36533084808946875, 0.36533084808946875, 0.36672879776328055, 0.36672879776328055, 0.36719478098788444, 0.36719478098788444, 0.36766076421248833, 0.36766076421248833, 0.3685927306616962, 0.3685927306616962, 0.36905871388630007, 0.36905871388630007, 0.369524697110904, 0.369524697110904, 0.3699906803355079, 0.3699906803355079, 0.3704566635601118, 0.3704566635601118, 0.37092264678471576, 0.37092264678471576, 0.3718546132339236, 0.3718546132339236, 0.37325256290773534, 0.37325256290773534, 0.37371854613233924, 0.37371854613233924, 0.3746505125815471, 0.3746505125815471, 0.375116495806151, 0.375116495806151, 0.37791239515377445, 0.37791239515377445, 0.3788443616029823, 0.3788443616029823, 0.37977632805219014, 0.37977632805219014, 0.38024231127679403, 0.38024231127679403, 0.3807082945013979, 0.3807082945013979, 0.38164026095060577, 0.38164026095060577, 0.3821062441752097, 0.3821062441752097, 0.3830382106244175, 0.3830382106244175, 0.38350419384902146, 0.38350419384902146, 0.38397017707362535, 0.38397017707362535, 0.38443616029822925, 0.38443616029822925, 0.3853681267474371, 0.3853681267474371, 0.38676607642124883, 0.38676607642124883, 0.38769804287045667, 0.38769804287045667, 0.3886300093196645, 0.3886300093196645, 0.3890959925442684, 0.3890959925442684, 0.39002795899347625, 0.39002795899347625, 0.39049394221808015, 0.39049394221808015, 0.39095992544268404, 0.39095992544268404, 0.391425908667288, 0.391425908667288, 0.39282385834109973, 0.39282385834109973, 0.3932898415657036, 0.3932898415657036, 0.3937558247903076, 0.3937558247903076, 0.3951537744641193, 0.3951537744641193, 0.3956197576887232, 0.3956197576887232, 0.39655172413793105, 0.39655172413793105, 0.39701770736253494, 0.39701770736253494, 0.39748369058713884, 0.39748369058713884, 0.3979496738117428, 0.3979496738117428, 0.39888164026095063, 0.39888164026095063, 0.3993476234855545, 0.3993476234855545, 0.40027958993476237, 0.40027958993476237, 0.40121155638397016, 0.4016775396085741, 0.4016775396085741, 0.402143522833178, 0.402143522833178, 0.40307548928238585, 0.40307548928238585, 0.40354147250698974, 0.40354147250698974, 0.4044734389561976, 0.4044734389561976, 0.4049394221808015, 0.4049394221808015, 0.4063373718546132, 0.4063373718546132, 0.40726933830382106, 0.40726933830382106, 0.4082013047530289, 0.4082013047530289, 0.4086672879776328, 0.4086672879776328, 0.40959925442684064, 0.40959925442684064, 0.41006523765144454, 0.41006523765144454, 0.4114631873252563, 0.4114631873252563, 0.412861136999068, 0.412861136999068, 0.41425908667287975, 0.41425908667287975, 0.4147250698974837, 0.4147250698974837, 0.4151910531220876, 0.4151910531220876, 0.41565703634669154, 0.41565703634669154, 0.41612301957129544, 0.41612301957129544, 0.4175209692451072, 0.4175209692451072, 0.41798695246971107, 0.41798695246971107, 0.418452935694315, 0.418452935694315, 0.4189189189189189, 0.4189189189189189, 0.4193849021435228, 0.4193849021435228, 0.42031686859273065, 0.42031686859273065, 0.4207828518173346, 0.4207828518173346, 0.42218080149114634, 0.42218080149114634, 0.42311276794035413, 0.42311276794035413, 0.4235787511649581, 0.4235787511649581, 0.424044734389562, 0.424044734389562, 0.4249767008387698, 0.4249767008387698, 0.42590866728797766, 0.42590866728797766, 0.42637465051258155, 0.42637465051258155, 0.4273066169617894, 0.4273066169617894, 0.4277726001863933, 0.4277726001863933, 0.4282385834109972, 0.4282385834109972, 0.4296365330848089, 0.4296365330848089, 0.4301025163094129, 0.4301025163094129, 0.43103448275862066, 0.43103448275862066, 0.4315004659832246, 0.4315004659832246, 0.4319664492078285, 0.4319664492078285, 0.43243243243243246, 0.43243243243243246, 0.43289841565703635, 0.43289841565703635, 0.43336439888164024, 0.43336439888164024, 0.434762348555452, 0.434762348555452, 0.4356943150046598, 0.4356943150046598, 0.4361602982292637, 0.4361602982292637, 0.43662628145386767, 0.43662628145386767, 0.4384902143522833, 0.4384902143522833, 0.43895619757688725, 0.43895619757688725, 0.43988816402609504, 0.43988816402609504, 0.440354147250699, 0.440354147250699, 0.4408201304753029, 0.4408201304753029, 0.4412861136999068, 0.4412861136999068, 0.4422180801491146, 0.4422180801491146, 0.4426840633737186, 0.4426840633737186, 0.44315004659832247, 0.44315004659832247, 0.4440820130475303, 0.4440820130475303, 0.44547996272134205, 0.44547996272134205, 0.44594594594594594, 0.44594594594594594, 0.44641192917054984, 0.44641192917054984, 0.4473438956197577, 0.4473438956197577, 0.4482758620689655, 0.4482758620689655, 0.4487418452935694, 0.4487418452935694, 0.45013979496738116, 0.45013979496738116, 0.4506057781919851, 0.4506057781919851, 0.45200372786579684, 0.45200372786579684, 0.45293569431500463, 0.45293569431500463, 0.4538676607642125, 0.4538676607642125, 0.4552656104380242, 0.4552656104380242, 0.4575955265610438, 0.4575955265610438, 0.4580615097856477, 0.4580615097856477, 0.45852749301025164, 0.45852749301025164, 0.45899347623485554, 0.45899347623485554, 0.4594594594594595, 0.4594594594594595, 0.4608574091332712, 0.4608574091332712, 0.461789375582479, 0.461789375582479, 0.46225535880708296, 0.46225535880708296, 0.46318732525629075, 0.46318732525629075, 0.4636533084808947, 0.4636533084808947, 0.46505125815470644, 0.46505125815470644, 0.4659832246039143, 0.4659832246039143, 0.4664492078285182, 0.4664492078285182, 0.46691519105312207, 0.46691519105312207, 0.4683131407269338, 0.4683131407269338, 0.46924510717614165, 0.46924510717614165, 0.4706430568499534, 0.4706430568499534, 0.47157502329916123, 0.47157502329916123, 0.47297297297297297, 0.47297297297297297, 0.47809878844361603, 0.47809878844361603, 0.47996272134203166, 0.47996272134203166, 0.4804287045666356, 0.4804287045666356, 0.4808946877912395, 0.4808946877912395, 0.48182665424044735, 0.48182665424044735, 0.483690587138863, 0.483690587138863, 0.4846225535880708, 0.4846225535880708, 0.4850885368126747, 0.4850885368126747, 0.48602050326188256, 0.48602050326188256, 0.48835041938490215, 0.48835041938490215, 0.48881640260950604, 0.48881640260950604, 0.4897483690587139, 0.4897483690587139, 0.4916123019571295, 0.4916123019571295, 0.4934762348555452, 0.4934762348555452, 0.4939422180801491, 0.4939422180801491, 0.49440820130475305, 0.49440820130475305, 0.49534016775396084, 0.49534016775396084, 0.4962721342031687, 0.4962721342031687, 0.4976700838769804, 0.4976700838769804, 0.49860205032618826, 0.49860205032618826, 0.49906803355079216, 0.49906803355079216, 0.4995340167753961, 0.4995340167753961, 0.5, 0.5, 0.5004659832246039, 0.5004659832246039, 0.5013979496738118, 0.5013979496738118, 0.5018639328984157, 0.5018639328984157, 0.5023299161230196, 0.5023299161230196, 0.5027958993476235, 0.5027958993476235, 0.5032618825722274, 0.5032618825722274, 0.5037278657968313, 0.5037278657968313, 0.5041938490214353, 0.5041938490214353, 0.5051258154706431, 0.5051258154706431, 0.505591798695247, 0.505591798695247, 0.5060577819198508, 0.5060577819198508, 0.5065237651444549, 0.5065237651444549, 0.5069897483690587, 0.5069897483690587, 0.5074557315936626, 0.5074557315936626, 0.5079217148182665, 0.5079217148182665, 0.5083876980428704, 0.5083876980428704, 0.5102516309412861, 0.5102516309412861, 0.51071761416589, 0.51071761416589, 0.5121155638397018, 0.5121155638397018, 0.5125815470643057, 0.5125815470643057, 0.5135135135135135, 0.5135135135135135, 0.5139794967381174, 0.5139794967381174, 0.5144454799627214, 0.5144454799627214, 0.5158434296365331, 0.5158434296365331, 0.516309412861137, 0.516309412861137, 0.516775396085741, 0.516775396085741, 0.5172413793103449, 0.5172413793103449, 0.5181733457595527, 0.5181733457595527, 0.5186393289841565, 0.5186393289841565, 0.5195712954333644, 0.5195712954333644, 0.5209692451071761, 0.5209692451071761, 0.52143522833178, 0.52143522833178, 0.5228331780055918, 0.5228331780055918, 0.5237651444547996, 0.5237651444547996, 0.5246971109040075, 0.5246971109040075, 0.5251630941286114, 0.5251630941286114, 0.5256290773532153, 0.5256290773532153, 0.527027027027027, 0.527027027027027, 0.5279589934762349, 0.5279589934762349, 0.5284249767008388, 0.5284249767008388, 0.5288909599254427, 0.5288909599254427, 0.5307548928238583, 0.5307548928238583, 0.5312208760484622, 0.5312208760484622, 0.5316868592730661, 0.5316868592730661, 0.532618825722274, 0.532618825722274, 0.5330848089468779, 0.5330848089468779, 0.5335507921714818, 0.5335507921714818, 0.5344827586206896, 0.5344827586206896, 0.5349487418452936, 0.5349487418452936, 0.5354147250698975, 0.5354147250698975, 0.5358807082945014, 0.5358807082945014, 0.5363466915191053, 0.5363466915191053, 0.5372786579683131, 0.5372786579683131, 0.5377446411929171, 0.5377446411929171, 0.5386766076421249, 0.5386766076421249, 0.5396085740913327, 0.5396085740913327, 0.5400745573159367, 0.5400745573159367, 0.5405405405405406, 0.5405405405405406, 0.5410065237651445, 0.5410065237651445, 0.5414725069897484, 0.5414725069897484, 0.5419384902143523, 0.5419384902143523, 0.5424044734389561, 0.5424044734389561, 0.543336439888164, 0.543336439888164, 0.5438024231127679, 0.5438024231127679, 0.5442684063373718, 0.5442684063373718, 0.5447343895619757, 0.5447343895619757, 0.5452003727865797, 0.5452003727865797, 0.5456663560111836, 0.5456663560111836, 0.5470643056849953, 0.5470643056849953, 0.5479962721342032, 0.5479962721342032, 0.5484622553588071, 0.5484622553588071, 0.548928238583411, 0.548928238583411, 0.5493942218080149, 0.5493942218080149, 0.5498602050326188, 0.5498602050326188, 0.5503261882572228, 0.5503261882572228, 0.5517241379310345, 0.5517241379310345, 0.5521901211556384, 0.5521901211556384, 0.5526561043802423, 0.5526561043802423, 0.5531220876048463, 0.5531220876048463, 0.5535880708294502, 0.5535880708294502, 0.5540540540540541, 0.5540540540540541, 0.554520037278658, 0.554520037278658, 0.5559179869524697, 0.5559179869524697, 0.5563839701770736, 0.5563839701770736, 0.5568499534016775, 0.5568499534016775, 0.5591798695246971, 0.5591798695246971, 0.559645852749301, 0.559645852749301, 0.5605778191985089, 0.5605778191985089, 0.5619757688723206, 0.5619757688723206, 0.5624417520969245, 0.5624417520969245, 0.5633737185461324, 0.5633737185461324, 0.5638397017707363, 0.5638397017707363, 0.5647716682199441, 0.5647716682199441, 0.565237651444548, 0.565237651444548, 0.5666356011183598, 0.5666356011183598, 0.5675675675675675, 0.5675675675675675, 0.5680335507921714, 0.5680335507921714, 0.5689655172413793, 0.5689655172413793, 0.5698974836905871, 0.5698974836905871, 0.570363466915191, 0.570363466915191, 0.5722273998136067, 0.5722273998136067, 0.5726933830382106, 0.5726933830382106, 0.5736253494874185, 0.5736253494874185, 0.5745573159366263, 0.5745573159366263, 0.5750232991612302, 0.5750232991612302, 0.5754892823858341, 0.5754892823858341, 0.575955265610438, 0.575955265610438, 0.5768872320596459, 0.5768872320596459, 0.5773532152842498, 0.5773532152842498, 0.5787511649580616, 0.5787511649580616, 0.5792171481826655, 0.5792171481826655, 0.5796831314072693, 0.5796831314072693, 0.5801491146318732, 0.5801491146318732, 0.5806150978564771, 0.5806150978564771, 0.5806150978564771, 0.5806150978564771, 0.581081081081081, 0.581081081081081, 0.581547064305685, 0.581547064305685, 0.5829450139794967, 0.5829450139794967, 0.5838769804287046, 0.5838769804287046, 0.5848089468779124, 0.5848089468779124, 0.5852749301025163, 0.5852749301025163, 0.5862068965517241, 0.5862068965517241, 0.5876048462255359, 0.5876048462255359, 0.5880708294501398, 0.5880708294501398, 0.5899347623485555, 0.5899347623485555, 0.5908667287977633, 0.5908667287977633, 0.5913327120223671, 0.5913327120223671, 0.5917986952469712, 0.5917986952469712, 0.592264678471575, 0.592264678471575, 0.5931966449207828, 0.5931966449207828, 0.5941286113699907, 0.5941286113699907, 0.5964585274930102, 0.5964585274930102, 0.597856477166822, 0.597856477166822, 0.5983224603914259, 0.5983224603914259, 0.5987884436160298, 0.5987884436160298, 0.5992544268406338, 0.5992544268406338, 0.6001863932898416, 0.6001863932898416, 0.6006523765144455, 0.6006523765144455, 0.6011183597390494, 0.6011183597390494, 0.6029822926374651, 0.6029822926374651, 0.6057781919850885, 0.6057781919850885, 0.6062441752096924, 0.6062441752096924, 0.6071761416589003, 0.6071761416589003, 0.6076421248835042, 0.6076421248835042, 0.6081081081081081, 0.6081081081081081, 0.608574091332712, 0.608574091332712, 0.6095060577819198, 0.6095060577819198, 0.6099720410065238, 0.6099720410065238, 0.6109040074557316, 0.6109040074557316, 0.6113699906803355, 0.6113699906803355, 0.6118359739049394, 0.6118359739049394, 0.6136999068033551, 0.6136999068033551, 0.6146318732525629, 0.6146318732525629, 0.6150978564771669, 0.6150978564771669, 0.6155638397017708, 0.6155638397017708, 0.6160298229263746, 0.6160298229263746, 0.6174277726001864, 0.6174277726001864, 0.6178937558247903, 0.6178937558247903, 0.6183597390493942, 0.6183597390493942, 0.6188257222739981, 0.6188257222739981, 0.619291705498602, 0.619291705498602, 0.6202236719478099, 0.6202236719478099, 0.6206896551724138, 0.6206896551724138, 0.6216216216216216, 0.6216216216216216, 0.6225535880708295, 0.6225535880708295, 0.6230195712954334, 0.6230195712954334, 0.6244175209692451, 0.6244175209692451, 0.625349487418453, 0.625349487418453, 0.6258154706430569, 0.6267474370922647, 0.6267474370922647, 0.6272134203168686, 0.6272134203168686, 0.6286113699906803, 0.6286113699906803, 0.6290773532152842, 0.6290773532152842, 0.6295433364398881, 0.6295433364398881, 0.6314072693383038, 0.6314072693383038, 0.6318732525629077, 0.6318732525629077, 0.6328052190121156, 0.6328052190121156, 0.6332712022367195, 0.6332712022367195, 0.6342031686859273, 0.6342031686859273, 0.6346691519105312, 0.6346691519105312, 0.6356011183597391, 0.6356011183597391, 0.636067101584343, 0.636067101584343, 0.6374650512581547, 0.6374650512581547, 0.6379310344827587, 0.6379310344827587, 0.6388630009319665, 0.6388630009319665, 0.6393289841565704, 0.6393289841565704, 0.6402609506057781, 0.6402609506057781, 0.6407269338303822, 0.6407269338303822, 0.641192917054986, 0.641192917054986, 0.6416589002795899, 0.6416589002795899, 0.6421248835041938, 0.6421248835041938, 0.6425908667287977, 0.6425908667287977, 0.6435228331780056, 0.6435228331780056, 0.6439888164026095, 0.6439888164026095, 0.6453867660764212, 0.6453867660764212, 0.6458527493010252, 0.6458527493010252, 0.6463187325256291, 0.6463187325256291, 0.646784715750233, 0.646784715750233, 0.6481826654240447, 0.6481826654240447, 0.6491146318732526, 0.6491146318732526, 0.6500465983224604, 0.6500465983224604, 0.6509785647716683, 0.6509785647716683, 0.6514445479962722, 0.6514445479962722, 0.65237651444548, 0.65237651444548, 0.6528424976700838, 0.6528424976700838, 0.6533084808946877, 0.6533084808946877, 0.6547064305684995, 0.6547064305684995, 0.6561043802423113, 0.6561043802423113, 0.6565703634669152, 0.6565703634669152, 0.657502329916123, 0.657502329916123, 0.6589002795899348, 0.6589002795899348, 0.6593662628145387, 0.6593662628145387, 0.6612301957129544, 0.6612301957129544, 0.6621621621621622, 0.6621621621621622, 0.6626281453867661, 0.6626281453867661, 0.6644920782851818, 0.6644920782851818, 0.6654240447343895, 0.6654240447343895, 0.6658900279589934, 0.6658900279589934, 0.6663560111835974, 0.6663560111835974, 0.6668219944082013, 0.6668219944082013, 0.6677539608574091, 0.6677539608574091, 0.6686859273066169, 0.6686859273066169, 0.6714818266542405, 0.6714818266542405, 0.6719478098788444, 0.6719478098788444, 0.6724137931034483, 0.6724137931034483, 0.6733457595526561, 0.6733457595526561, 0.67381174277726, 0.67381174277726, 0.674277726001864, 0.674277726001864, 0.6747437092264679, 0.6747437092264679, 0.6756756756756757, 0.6756756756756757, 0.6761416589002796, 0.6761416589002796, 0.6775396085740913, 0.6775396085740913, 0.6780055917986952, 0.6780055917986952, 0.678937558247903, 0.678937558247903, 0.6798695246971109, 0.6798695246971109, 0.6821994408201305, 0.6821994408201305, 0.6835973904939422, 0.6835973904939422, 0.6845293569431501, 0.6845293569431501, 0.684995340167754, 0.684995340167754, 0.6854613233923579, 0.6854613233923579, 0.6859273066169618, 0.6859273066169618, 0.6873252562907736, 0.6873252562907736, 0.690121155638397, 0.690121155638397, 0.6905871388630009, 0.6905871388630009, 0.6910531220876048, 0.6910531220876048, 0.6915191053122087, 0.6915191053122087, 0.6924510717614166, 0.6924510717614166, 0.6933830382106244, 0.6933830382106244, 0.6938490214352283, 0.6938490214352283, 0.6947809878844362, 0.6947809878844362, 0.695712954333644, 0.695712954333644, 0.6961789375582479, 0.6961789375582479, 0.6966449207828518, 0.6966449207828518, 0.6975768872320597, 0.6975768872320597, 0.6985088536812675, 0.6985088536812675, 0.6994408201304753, 0.6994408201304753, 0.6999068033550793, 0.6999068033550793, 0.7003727865796832, 0.7003727865796832, 0.700838769804287, 0.700838769804287, 0.701304753028891, 0.701304753028891, 0.7022367194780987, 0.7022367194780987, 0.7027027027027027, 0.7027027027027027, 0.7031686859273066, 0.7031686859273066, 0.7036346691519105, 0.7036346691519105, 0.7059645852749301, 0.7059645852749301, 0.7078285181733458, 0.7078285181733458, 0.7087604846225536, 0.7087604846225536, 0.7092264678471575, 0.7092264678471575, 0.7096924510717614, 0.7096924510717614, 0.7101584342963654, 0.7101584342963654, 0.7106244175209693, 0.7106244175209693, 0.7115563839701771, 0.7115563839701771, 0.712022367194781, 0.712022367194781, 0.7124883504193849, 0.7124883504193849, 0.7134203168685928, 0.7134203168685928, 0.7176141658900279, 0.7176141658900279, 0.7180801491146319, 0.7180801491146319, 0.7194780987884436, 0.7194780987884436, 0.7199440820130475, 0.7199440820130475, 0.7204100652376515, 0.7204100652376515, 0.7208760484622554, 0.7208760484622554, 0.7213420316868593, 0.7213420316868593, 0.7222739981360671, 0.7222739981360671, 0.7236719478098789, 0.7236719478098789, 0.7250698974836906, 0.7250698974836906, 0.7269338303821062, 0.7269338303821062, 0.727865796831314, 0.727865796831314, 0.7287977632805219, 0.7287977632805219, 0.7292637465051258, 0.7292637465051258, 0.7297297297297297, 0.7297297297297297, 0.7301957129543336, 0.7301957129543336, 0.7306616961789375, 0.7306616961789375, 0.7315936626281454, 0.7315936626281454, 0.7320596458527493, 0.7320596458527493, 0.7325256290773532, 0.7325256290773532, 0.733923578751165, 0.733923578751165, 0.7367194780987885, 0.7367194780987885, 0.7376514445479962, 0.7376514445479962, 0.7381174277726001, 0.7381174277726001, 0.7385834109972041, 0.7385834109972041, 0.739049394221808, 0.739049394221808, 0.7404473438956197, 0.7404473438956197, 0.7409133271202236, 0.7409133271202236, 0.7413793103448276, 0.7413793103448276, 0.7418452935694315, 0.7418452935694315, 0.7423112767940354, 0.7423112767940354, 0.7437092264678472, 0.7437092264678472, 0.7441752096924511, 0.7441752096924511, 0.744641192917055, 0.744641192917055, 0.7451071761416589, 0.7451071761416589, 0.7460391425908667, 0.7460391425908667, 0.7469711090400746, 0.7469711090400746, 0.7474370922646785, 0.7474370922646785, 0.7483690587138863, 0.7483690587138863, 0.7493010251630942, 0.7493010251630942, 0.7516309412861137, 0.7516309412861137, 0.7525629077353215, 0.7525629077353215, 0.7530288909599254, 0.7530288909599254, 0.7534948741845293, 0.7534948741845293, 0.7539608574091333, 0.7539608574091333, 0.7544268406337372, 0.7544268406337372, 0.7558247903075489, 0.7558247903075489, 0.7581547064305685, 0.7581547064305685, 0.7586206896551724, 0.7586206896551724, 0.7590866728797764, 0.7590866728797764, 0.7595526561043803, 0.7595526561043803, 0.7600186393289842, 0.7600186393289842, 0.7604846225535881, 0.7604846225535881, 0.760950605778192, 0.760950605778192, 0.7614165890027959, 0.7614165890027959, 0.7618825722273999, 0.7618825722273999, 0.7623485554520038, 0.7623485554520038, 0.7628145386766076, 0.7628145386766076, 0.7637465051258154, 0.7637465051258154, 0.7646784715750233, 0.7646784715750233, 0.7651444547996272, 0.7651444547996272, 0.7670083876980429, 0.7670083876980429, 0.7684063373718546, 0.7684063373718546, 0.7688723205964585, 0.7688723205964585, 0.7707362534948742, 0.7707362534948742, 0.7712022367194781, 0.7712022367194781, 0.771668219944082, 0.771668219944082, 0.772134203168686, 0.772134203168686, 0.7730661696178938, 0.7730661696178938, 0.7739981360671015, 0.7739981360671015, 0.7753960857409133, 0.7753960857409133, 0.7758620689655172, 0.7758620689655172, 0.7763280521901211, 0.7763280521901211, 0.776794035414725, 0.776794035414725, 0.7781919850885368, 0.7781919850885368, 0.7795899347623485, 0.7795899347623485, 0.7805219012115564, 0.7805219012115564, 0.7814538676607642, 0.7814538676607642, 0.7837837837837838, 0.7837837837837838, 0.7842497670083877, 0.7842497670083877, 0.7847157502329916, 0.7847157502329916, 0.7856477166821995, 0.7856477166821995, 0.7865796831314072, 0.7865796831314072, 0.7870456663560111, 0.7870456663560111, 0.7875116495806151, 0.7875116495806151, 0.787977632805219, 0.787977632805219, 0.7884436160298229, 0.7884436160298229, 0.7893755824790307, 0.7893755824790307, 0.7898415657036346, 0.7898415657036346, 0.7903075489282386, 0.7903075489282386, 0.7917054986020503, 0.7917054986020503, 0.793569431500466, 0.793569431500466, 0.7945013979496738, 0.7945013979496738, 0.7949673811742777, 0.7949673811742777, 0.7954333643988817, 0.7954333643988817, 0.7958993476234856, 0.7958993476234856, 0.7963653308480895, 0.7963653308480895, 0.7972972972972973, 0.7972972972972973, 0.799161230195713, 0.799161230195713, 0.8000931966449207, 0.8000931966449207, 0.8005591798695247, 0.8005591798695247, 0.8010251630941286, 0.8010251630941286, 0.8014911463187325, 0.8014911463187325, 0.8024231127679403, 0.8024231127679403, 0.8033550792171482, 0.8033550792171482, 0.804287045666356, 0.804287045666356, 0.8047530288909599, 0.8047530288909599, 0.8070829450139795, 0.8070829450139795, 0.8080149114631874, 0.8080149114631874, 0.8084808946877913, 0.8084808946877913, 0.8089468779123952, 0.8089468779123952, 0.8094128611369991, 0.8094128611369991, 0.809878844361603, 0.809878844361603, 0.8103448275862069, 0.8103448275862069, 0.8122087604846225, 0.8122087604846225, 0.8126747437092264, 0.8126747437092264, 0.8140726933830382, 0.8140726933830382, 0.8159366262814539, 0.8159366262814539, 0.8164026095060578, 0.8164026095060578, 0.8173345759552656, 0.8173345759552656, 0.8178005591798695, 0.8178005591798695, 0.8182665424044734, 0.8182665424044734, 0.8187325256290774, 0.8187325256290774, 0.8191985088536813, 0.8191985088536813, 0.8196644920782852, 0.8196644920782852, 0.820596458527493, 0.820596458527493, 0.8215284249767009, 0.8215284249767009, 0.8229263746505125, 0.8229263746505125, 0.8233923578751164, 0.8233923578751164, 0.8238583410997204, 0.8238583410997204, 0.8243243243243243, 0.8243243243243243, 0.825722273998136, 0.825722273998136, 0.8275862068965517, 0.8275862068965517, 0.8289841565703635, 0.8289841565703635, 0.8299161230195713, 0.8299161230195713, 0.8313140726933831, 0.8313140726933831, 0.8322460391425909, 0.8322460391425909, 0.8331780055917987, 0.8331780055917987, 0.8341099720410066, 0.8341099720410066, 0.8345759552656105, 0.8345759552656105, 0.8359739049394221, 0.8359739049394221, 0.8378378378378378, 0.8378378378378378, 0.8383038210624417, 0.8383038210624417, 0.8387698042870456, 0.8387698042870456, 0.8392357875116496, 0.8392357875116496, 0.8406337371854613, 0.8406337371854613, 0.8410997204100652, 0.8410997204100652, 0.8415657036346692, 0.8415657036346692, 0.842497670083877, 0.842497670083877, 0.8438956197576887, 0.8438956197576887, 0.8462255358807083, 0.8462255358807083, 0.8471575023299162, 0.8471575023299162, 0.84762348555452, 0.84762348555452, 0.848089468779124, 0.848089468779124, 0.8485554520037278, 0.8485554520037278, 0.8490214352283317, 0.8490214352283317, 0.8499534016775396, 0.8499534016775396, 0.8527493010251631, 0.8527493010251631, 0.8546132339235788, 0.8546132339235788, 0.8560111835973905, 0.8560111835973905, 0.8569431500465983, 0.8569431500465983, 0.8574091332712023, 0.8574091332712023, 0.858807082945014, 0.858807082945014, 0.8592730661696178, 0.8592730661696178, 0.8597390493942219, 0.8597390493942219, 0.8606710158434296, 0.8606710158434296, 0.8616029822926374, 0.8616029822926374, 0.8625349487418453, 0.8625349487418453, 0.8662628145386766, 0.8662628145386766, 0.8667287977632805, 0.8667287977632805, 0.8681267474370923, 0.8681267474370923, 0.8690587138863001, 0.8690587138863001, 0.869990680335508, 0.869990680335508, 0.8709226467847158, 0.8709226467847158, 0.8713886300093197, 0.8713886300093197, 0.8737185461323392, 0.8737185461323392, 0.874650512581547, 0.874650512581547, 0.875116495806151, 0.875116495806151, 0.8755824790307549, 0.8755824790307549, 0.8760484622553588, 0.8760484622553588, 0.8765144454799627, 0.8765144454799627, 0.8779123951537745, 0.8779123951537745, 0.8797763280521901, 0.8797763280521901, 0.8802423112767941, 0.8802423112767941, 0.880708294501398, 0.880708294501398, 0.8816402609506058, 0.8816402609506058, 0.8821062441752097, 0.8821062441752097, 0.8825722273998136, 0.8825722273998136, 0.8835041938490215, 0.8835041938490215, 0.8839701770736254, 0.8839701770736254, 0.8844361602982292, 0.8844361602982292, 0.8849021435228331, 0.8849021435228331, 0.885834109972041, 0.885834109972041, 0.8863000931966449, 0.8863000931966449, 0.8867660764212488, 0.8867660764212488, 0.8876980428704566, 0.8876980428704566, 0.8881640260950606, 0.8881640260950606, 0.8886300093196645, 0.8886300093196645, 0.8890959925442684, 0.8890959925442684, 0.8895619757688723, 0.8895619757688723, 0.8918918918918919, 0.8918918918918919, 0.8928238583410997, 0.8928238583410997, 0.8942218080149115, 0.8942218080149115, 0.8946877912395154, 0.8946877912395154, 0.8960857409133272, 0.8960857409133272, 0.896551724137931, 0.896551724137931, 0.8970177073625349, 0.8970177073625349, 0.8974836905871388, 0.8974836905871388, 0.8979496738117427, 0.8979496738117427, 0.8984156570363467, 0.8984156570363467, 0.8988816402609506, 0.8988816402609506, 0.8993476234855545, 0.8993476234855545, 0.9016775396085741, 0.9016775396085741, 0.902143522833178, 0.902143522833178, 0.9030754892823858, 0.9030754892823858, 0.9044734389561976, 0.9044734389561976, 0.9054054054054054, 0.9054054054054054, 0.9063373718546133, 0.9063373718546133, 0.9072693383038211, 0.9072693383038211, 0.907735321528425, 0.907735321528425, 0.9086672879776329, 0.9086672879776329, 0.9100652376514445, 0.9100652376514445, 0.9105312208760484, 0.9105312208760484, 0.9119291705498602, 0.9119291705498602, 0.912861136999068, 0.912861136999068, 0.9133271202236719, 0.9133271202236719, 0.9142590866728798, 0.9142590866728798, 0.9147250698974837, 0.9147250698974837, 0.9151910531220876, 0.9151910531220876, 0.9156570363466915, 0.9156570363466915, 0.9161230195712954, 0.9161230195712954, 0.9165890027958994, 0.9165890027958994, 0.9170549860205033, 0.9170549860205033, 0.9175209692451072, 0.9175209692451072, 0.918452935694315, 0.918452935694315, 0.918918918918919, 0.918918918918919, 0.9193849021435229, 0.9193849021435229, 0.9203168685927307, 0.9203168685927307, 0.9207828518173345, 0.9207828518173345, 0.9217148182665424, 0.9217148182665424, 0.924044734389562, 0.924044734389562, 0.9263746505125815, 0.9263746505125815, 0.9268406337371855, 0.9268406337371855, 0.9282385834109972, 0.9282385834109972, 0.9315004659832246, 0.9315004659832246, 0.9319664492078286, 0.9319664492078286, 0.9333643988816402, 0.9333643988816402, 0.9338303821062441, 0.9338303821062441, 0.9352283317800559, 0.9352283317800559, 0.9356943150046598, 0.9356943150046598, 0.9366262814538676, 0.9366262814538676, 0.9370922646784716, 0.9370922646784716, 0.9412861136999068, 0.9412861136999068, 0.9422180801491147, 0.9422180801491147, 0.9426840633737186, 0.9426840633737186, 0.9440820130475303, 0.9440820130475303, 0.9445479962721341, 0.9445479962721341, 0.945479962721342, 0.945479962721342, 0.9459459459459459, 0.9459459459459459, 0.9468779123951537, 0.9468779123951537, 0.9482758620689655, 0.9482758620689655, 0.9501397949673812, 0.9501397949673812, 0.9506057781919851, 0.9506057781919851, 0.9515377446411929, 0.9515377446411929, 0.9520037278657968, 0.9520037278657968, 0.9529356943150047, 0.9529356943150047, 0.9534016775396086, 0.9534016775396086, 0.9543336439888164, 0.9543336439888164, 0.9547996272134203, 0.9547996272134203, 0.9557315936626282, 0.9557315936626282, 0.9561975768872321, 0.9561975768872321, 0.956663560111836, 0.956663560111836, 0.9575955265610439, 0.9575955265610439, 0.9580615097856477, 0.9580615097856477, 0.9589934762348555, 0.9589934762348555, 0.9594594594594594, 0.9594594594594594, 0.9599254426840633, 0.9599254426840633, 0.9603914259086673, 0.9603914259086673, 0.9608574091332712, 0.9608574091332712, 0.9613233923578751, 0.9613233923578751, 0.9622553588070829, 0.9622553588070829, 0.9627213420316869, 0.9627213420316869, 0.9659832246039143, 0.9659832246039143, 0.9664492078285182, 0.9664492078285182, 0.9669151910531221, 0.9669151910531221, 0.967381174277726, 0.967381174277726, 0.96784715750233, 0.96784715750233, 0.9687791239515378, 0.9687791239515378, 0.9697110904007455, 0.9697110904007455, 0.9701770736253494, 0.9701770736253494, 0.9706430568499534, 0.9706430568499534, 0.9711090400745573, 0.9711090400745573, 0.9720410065237651, 0.9720410065237651, 0.972972972972973, 0.972972972972973, 0.9734389561975769, 0.9734389561975769, 0.9739049394221808, 0.9739049394221808, 0.9748369058713886, 0.9748369058713886, 0.9753028890959925, 0.9753028890959925, 0.9757688723205965, 0.9757688723205965, 0.9762348555452004, 0.9762348555452004, 0.9767008387698043, 0.9767008387698043, 0.9771668219944082, 0.9771668219944082, 0.9776328052190121, 0.9776328052190121, 0.9780987884436161, 0.9780987884436161, 0.97856477166822, 0.97856477166822, 0.9790307548928239, 0.9790307548928239, 0.9808946877912396, 0.9808946877912396, 0.9813606710158435, 0.9813606710158435, 0.9818266542404473, 0.9818266542404473, 0.9827586206896551, 0.9827586206896551, 0.983224603914259, 0.983224603914259, 0.9846225535880708, 0.9846225535880708, 0.9855545200372786, 0.9855545200372786, 0.9874184529356943, 0.9874184529356943, 0.9883504193849021, 0.9883504193849021, 0.98928238583411, 0.98928238583411, 0.9897483690587139, 0.9897483690587139, 0.9902143522833178, 0.9902143522833178, 0.9906803355079217, 0.9906803355079217, 0.9911463187325257, 0.9911463187325257, 0.9920782851817335, 0.9920782851817335, 0.9939422180801492, 0.9939422180801492, 0.9953401677539608, 0.9953401677539608, 0.9972041006523765, 0.9972041006523765, 0.9976700838769804, 0.9976700838769804, 1.0]]], "AUC": [0.4877981298161071], "f1_score": [0.872444011684518], "kappa": [0.7763179937885798], "accuracy": [0.890146750524109]}',1643539882.197,1643539885.82368,1,'{"mode": "split", "train_partition": 10, "target": ["Class"], "columns": ["At1", "At2"], "random_seed": null, "alg_type": "Classification"}');
INSERT INTO "experiments" VALUES (11,1,'sklearn.ensemble.RandomForestClassifier','{"n_estimators":100,"criterion":"gini","min_samples_split":2,"min_samples_leaf":1,"min_weight_fraction_leaf":0,"min_impurity_decrease":0,"bootstrap":true,"oob_score":false}',NULL,NULL,'14_banana_norm.arff','{"confussion_matrix": [[[2361, 267], [306, 1836]]], "ROC": [[[0.0, 0.1069094304388422, 0.19140989729225025, 0.2572362278244631, 0.31746031746031744, 0.36554621848739494, 0.4169000933706816, 0.46405228758169936, 0.49486461251167135, 0.5228758169934641, 0.5494864612511672, 0.5765639589169, 0.6013071895424836, 0.6171802054154996, 0.6358543417366946, 0.6563958916900093, 0.6722689075630253, 0.688608776844071, 0.7044817927170869, 0.7273576097105509, 0.7413632119514473, 0.7553688141923436, 0.7665732959850607, 0.7787114845938375, 0.7889822595704948, 0.8029878618113913, 0.8132586367880486, 0.8197945845004668, 0.8281979458450047, 0.8366013071895425, 0.8445378151260504, 0.8580765639589168, 0.8674136321195145, 0.8814192343604108, 0.8898225957049486, 0.8944911297852475, 0.9024276377217554, 0.9080298786181139, 0.919234360410831, 0.9243697478991597, 0.9313725490196079, 0.938375350140056, 0.9449112978524743, 0.9491129785247432, 0.9537815126050421, 0.9570494864612512, 0.9663865546218487, 0.9719887955182073, 0.9827264239028944, 0.9897292250233427, 0.9943977591036415, 1.0], [0.0, 0.158675799086758, 0.289193302891933, 0.3877473363774734, 0.4668949771689498, 0.519406392694064, 0.5509893455098934, 0.5795281582952816, 0.5996955859969558, 0.6339421613394216, 0.6529680365296804, 0.667427701674277, 0.680365296803653, 0.7005327245053272, 0.7184170471841704, 0.7302130898021308, 0.7416286149162862, 0.7530441400304414, 0.7636986301369864, 0.7728310502283106, 0.7853881278538812, 0.79337899543379, 0.8059360730593608, 0.8143074581430746, 0.8257229832572298, 0.8340943683409436, 0.8401826484018264, 0.8496955859969558, 0.8557838660578386, 0.860730593607306, 0.867579908675799, 0.8770928462709284, 0.8843226788432268, 0.8896499238964992, 0.8987823439878234, 0.9067732115677322, 0.9113394216133942, 0.9181887366818874, 0.923896499238965, 0.9299847792998478, 0.9364535768645358, 0.9402587519025876, 0.9471080669710806, 0.952054794520548, 0.9577625570776256, 0.9684170471841704, 0.973744292237443, 0.9809741248097412, 0.9855403348554034, 0.9920091324200914, 0.9973363774733638, 1.0]]], "AUC": [0.5738823941550238], "f1_score": [0.891784702549575], "kappa": [0.7568188260616806], "accuracy": [0.879874213836478]}',1643540735.4473,1643540736.7226,1,'{"mode": "split", "train_partition": 10, "target": ["Class"], "columns": ["At1", "At2"], "random_seed": null, "alg_type": "Classification"}');
INSERT INTO "filters" VALUES (1,'weka.filters.supervised.instance.SMOTE','SMOTE','Balance','{"percentage":{"type":"integer","default":100,"help":"  Specifies percentage of SMOTE instances to create.","min":1,"command":"-P"},"nearest_neighbors":{"type":"integer","default":5,"help":"Specifies the number of nearest neighbors to use.","min":1,"command":"-K"},"seed":{"type":"integer","default":null,"help":"Random number seed.","min":1,"max":2147483647,"command":"-S"}}','weka');
INSERT INTO "filters" VALUES (2,'weka.filters.supervised.instance.SpreadSubsample','Spread Subsample','Balance','{"distribution_spread":{"type":"integer","default":0,"help":"  The maximum class distribution spread. 0 = no maximum spread, 1 = uniform distribution, 10 = allow at most a 10: ratio between the classes","min":0,"max":10,"command":"-M"},"adjust_weights":{"type":"boolean","default":false,"help":" Adjust weights so that total weight per class is maintained. Individual instance weighting is not preserved. (default no weights adjustment)","command":"-W"},"max_count":{"type":"integer","default":0,"help":" The maximum count for any class value (default 0 = unlimited).","min":0,"command":"-X"},"seed":{"type":"integer","default":null,"help":"Random number seed.","min":1,"max":2147483647,"command":"-S"}}','weka');
INSERT INTO "filters" VALUES (3,'weka.filters.supervised.instance.InstanceSelection','Instance Selection','Instance Selection','{"nearest_neighbours":{"type":"integer","default":1,"help":"The number of nearest neighbours","min":1,"command":"-K"},"type_of_solver":{"type":"string","default":"CNN","help":" Adjust weights so that total weight per class is maintained. Individual instance weighting is not preserved. (default no weights adjustment)","command":"-T","options":["CNN","RNN","MSS","ICF","LSB","LSS"]}}','weka');
INSERT INTO "filters" VALUES (4,'weka.filters.supervised.instance.DIS','Democratic Instance Selection','Instance Selection','{"nearest_neighbours":{"type":"integer","default":1,"help":"Specifies the number of nearest neighbours","min":1,"command":"-K"},"round_number":{"type":"integer","default":10,"help":"Round number","min":1,"command":"-R"},"number_of_bins":{"type":"integer","default":5,"help":"Number of bins","min":1,"command":"-S"},"type_of_partitioning":{"type":"integer","default":1,"help":"Set type of partitioning (default: 1)\n 1 = Grand\n 2 = Random Resample","min":1,"max":2,"command":"-P"},"angle_variability":{"type":"integer","default":1,"help":"Angle variability","min":1,"command":"-V"},"resample_percentage":{"type":"integer","default":25,"help":"Angle variability","min":1,"command":"-M"},"value_of_alpha":{"type":"float","default":7.5e-1,"help":"Value of alpha","min":0,"command":"-A"},"percentage_error":{"type":"integer","default":1,"help":"Instances percentage to calculate the error","min":1,"command":"-E"},"type_of_solver":{"type":"string","default":"CNN","help":" Adjust weights so that total weight per class is maintained. Individual instance weighting is not preserved. (default no weights adjustment)","command":"-T","options":["CNN","RNN","MSS","ICF","LSB","LSS"]}}','weka');
INSERT INTO "filters" VALUES (5,'imblearn.over_sampling.SMOTE','SMOTE (sklearn)','Balance','{"sampling_strategy":{"type":"string","default":"autoy","help":"specify the class targeted by the resampling. The number of samples in the different classes will be equalized. Possible choices are:\n''minority'': resample only the minority class;\n''not minority'': resample all classes but the minority class;\n''not majority'': resample all classes but the majority class;\n''all'': resample all classes;\n''auto'': equivalent to ''not majority''.","options":["auto","not_majurity","minority","not minority","all"]},"random_state":{"type":"integer","default":null,"help":"The seed of the pseudo random number generator to use when shuffling the data.","min":0,"max":4294967295},"k_neighbors":{"type":"integer","default":5,"help":"Number of nearest neighbours to used to construct synthetic samples.","min":1}}','sklearn');
INSERT INTO "filters" VALUES (6,'imblearn.under_sampling.NearMiss','NearMiss','Balance','{"sampling_strategy":{"type":"string","default":"autoy","help":"specify the class targeted by the resampling. The number of samples in the different classes will be equalized. Possible choices are:\n''minority'': resample only the minority class;\n''not minority'': resample all classes but the minority class;\n''not majority'': resample all classes but the majority class;\n''all'': resample all classes;\n''auto'': equivalent to ''not minority''.","options":["auto","not_majurity","minority","not minority","all"]},"varsion":{"type":"integer","default":1,"help":"Version of the NearMiss to use. Possible values are 1, 2 or 3.","min":1,"max":3},"n_neighbors":{"type":"integer","default":3,"help":"Size of the neighbourhood to consider to compute the average distance to the minority point samples.","min":1},"n_neighbors_ver3":{"type":"integer","default":3,"help":"NearMiss-3 algorithm start by a phase of re-sampling. This parameter correspond to the number of neighbours selected create the subset in which the selection will be performed.","min":1}}','sklearn');
COMMIT;
